\documentclass[class=co432,notes,tikz]{agony}

\title{CO 432 Spring 2025: Lecture Notes}

\begin{document}
\renewcommand{\contentsname}{CO 432 Spring 2025:\\{\huge Lecture Notes}}
\thispagestyle{firstpage}
\tableofcontents

Lecture notes taken, unless otherwise specified,
by myself during the Spring 2025 offering of CO 432,
taught by Vijay Bhattiprolu.

\begin{multicols}{2}
  \listoflecture
\end{multicols}

\chapter{Entropy}

\begin{notation}
  I will be using my usual \LaTeX{} typesetting conventions:
  \begin{itemize}[nosep]
    \item $[n]$ means the set $\{1,2,\dotsc,n\}$
    \item $\bits*$ means the set of bitstrings of arbitrary length (i.e., the Kleene star)
    \item $\sum_i$ is implicitly $\sum_{i=1}^n$
    \item $\rv A, \rv B, \dotsc, \rv Z$ are random variables (in sans-serif)
    \item $\X \sim (p_1,p_2,\dotsc,p_n)$ means $\X$ is a
          discrete random variable with $n$ outcomes
          such that $\Pr[\X = 1] = p_1$, $\Pr[\X=2] = p_2$, etc.
          (abbreviate further as $\X \sim (p_i)$)
  \end{itemize}
\end{notation}

\section{Definition}
\lecture{May 6}

\textrule{$\downarrow$ Lecture 1 adapted from Arthur $\downarrow$}

\begin{defn}[entropy]
  For a random variable $\X \sim (p_i)$,
  the \term*{entropy} $H(\X)$ is
  \[ H(\X) = -\sum_i p_i \log p_i = \sum_i p_i \log \frac{1}{p_i}. \]
\end{defn}

\begin{convention}
  By convention, we usually use $\log_2$.
  Also, we define entropy such that $\log_2(0) = 0$ so that
  impossible values do not break the formula.
\end{convention}

\begin{example}
  If $\X$ takes on the values $a$, $b$, $c$, $d$
  with probabilities 1, 0, 0, 0, respectively, then $H(\X) = 1 \log 1 = 0$.

  If $\X$ takes on those values instead with probabilities
  $\frac12$, $\frac14$, $\frac18$, $\frac18$, respectively,
  then $H(\X) = \frac74$.
\end{example}

\begin{fact}
  $H(\X) = 0$ if and only if $\X$ is a constant.
\end{fact}
\begin{prf}
  Suppose $\X$ is constant. Then, $H(\X) = 1 \log 1 = 0$.

  Suppose $H(\X) = 0$.
  Probabilities are in $[0,1]$, so $p_i \log \frac{1}{p_i} \geq 0$.
  Since $H(\X) = \sum_i p_i \log \frac{1}{p_i} = 0$
  and each term is non-negative, each term must be zero.
  Thus, each $p_i$ is either 0 or 1.
  We cannot have $\sum p_i > 1$, so exactly one $p_i = 1$ and the rest are zero.
  That is, $\X$ is constant.
\end{prf}

\begin{theorem}[Jensen's inequality]\label{thm:jensen}
  Let $f : \R \to \R$ be concave. That is,
  for any $a$ and $b$ in the domain of $f$ and $\lambda \in [0,1)$,
  $f(\lambda a + (1-\lambda)b) \geq \lambda f(a) + (1-\lambda)f(b)$.
  For any discrete random variable $\X$,
  \[ \E[f(\X)] \leq f(\E[\X]) \]
\end{theorem}
\begin{prf}
  Consider a random variable $\X$ with two values $a$ and $b$,
  each with probabilities $\lambda$ and $1-\lambda$.
  Then, notice that
  \[ \E[f(\X)] = \lambda f(a) + (1-\lambda) f(b) \leq f(\lambda a + (1-\lambda)b) = f(\E[\X]) \]
  by convexity of $f$.

  TODO: This can be generalized by induction.
\end{prf}

\begin{fact}
  Assume $\X$ is supported on $[n]$. Then, $0 \leq H(\X) \leq \log n$.
\end{fact}
\begin{prf}
  Start by claiming without proof that $\log n$ is concave, so we can apply
  \nameref{thm:jensen}.

  Let $\X' = \frac{1}{p_i}$ with probability $p_i$. Then,
  \begin{align*}
    H(\X) & = \sum_i p_i \log \frac{1}{p_i}    \\
          & = \E\qty[\log(\X')]                \\
          & \leq \log(\E[\X'])                 \\
          & = \log\qty(\sum p_i \frac{1}{p_i}) \\
          & = \log n \qedhere
  \end{align*}
\end{prf}

It is not a coincidence that $\log_2 n$ is the minimum number of bits to encode $[n]$.

\section{Entropy as expected surprise}

We want $S : [0,1] \to [0,\infty)$ to capture how ``surprised''
we are $S(p)$ that an event with probability $p$ happens.
We want to show that under some natural assumptions,
this is the only function we could have defined as entropy.
In particular:

\begin{enumerate}
  \item $S(1) = 0$, a certainty should not be surprising
  \item $S(q) > S(p)$ if $p > q$, less probable should be more surprising
  \item $S(p)$ is continuous in $p$
  \item $S(pq) =  S(p) + S(q)$, surprise should add for independent events.
        That is, if I see something twice, I should be twice as surprised.
\end{enumerate}

\textrule{$\uparrow$ Lecture 1 adapted from Arthur $\uparrow$}
\lecture{May 8}

\begin{prop}
  If $S(p)$ satisfies these 4 axioms, then $S(p)=c\cdot \log_2(1/p)$ for some $c > 0$.
\end{prop}
\begin{prf}
  Suppose a function $S : [0,1] \to [0,\infty)$ exists satisfying the axioms.
  Let $c := S(\frac12) > 0$.

  By axiom 4 (addition), $S(\frac{1}{2^k}) = kS(\frac12)$.
  Likewise, $S(\frac{1}{2^{1/k}}\cdots\frac{1}{2^{1/k}})
    = S(\frac{1}{2^{1/k}}) + \dotsb + S(\frac{1}{2^{1/k}}) = kS(\frac{1}{2^{1/k}})$.

  Then, $S(\frac{1}{2^{m/n}}) = \frac{m}{n}S(\frac12) = \frac{m}{n}\cdot c$
  for any rational $m/n$.

  By axiom 3 (continuity), $S(\frac{1}{2^z}) = c \cdot z$ for all $z \in [0,\infty)$
  because the rationals are dense in the reals.
  In particular, for any $p \in [0,1]$,
  we can write $p = \frac{1}{2^z}$ for $z = \log_2(1/p)$
  and we get \[ S\qty(p) = S\qty(\frac{1}{2^z}) = c \cdot z = c \cdot \log_2(1/p) \]
  as desired.
\end{prf}

We can now view entropy as expected surprise. In particular,
\[ \sum_i p_i \log_2\frac{1}{p_i} = \E_{x \sim \X}\qty[S(p_x)] \]
for a random variable $\X = i$ with probability $p_i$.

\section{Entropy as optimal lossless data compression}

Suppose we are trying to compress a string consisting of $n$
symbols drawn from some distribution.

\begin{restatable}{problem}{bitproblem}
  What is the expected number of bits you need to store the results of $n$ independent samples
  of a random variable $\X$?
\end{restatable}

We will show this is $nH(\X)$.

Notice that we assume that the symbols we are drawn \uline{independently},
which is violated by almost all data we actually care about.

\begin{restatable}{definition}{defcode}
  Let $C : \Sigma \to (\Sigma')^*$ be a code.
  We say $C$ is a \term[code!uniquely decodable]{uniquely decodable} code (UDC) if there does not exist
  a collision $x, y \in \Sigma^*$,
  with identical encoding $C(x_1)C(x_2)\cdots C(x_k) = C(y_1)C(y_2)\cdots C(y_{k'})$.

  Also, $C$ is \term[code!prefix-free]{prefix-free} (sometimes called \term*{instantaneous})
  if for any distinct $x,y \in \Sigma$, $C(x)$ is not a prefix of $C(y)$.
\end{restatable}

\begin{prop}
  Prefix-freeness is sufficient for unique decodability.
\end{prop}

\begin{example}
  Let $C : \{A,B,C,D\} \to \{0,1\}^*$ where
  $C(A) = 11$, $C(B) = 101$, $C(C) = 100$, and $C(D) = 00$.
  Then, $C$ is prefix-free and uniquely decodable.

  We can easily parse $1011100001100$ unambiguously as $101.11.00.00.11.00$
  ($BADDAD$).
\end{example}

Recall from CS 240 that a prefix-free code is equivalent to a trie,
and we can decode it by traversing the trie in linear time.

\begin{theorem}[Kraft's inequality]\label{thm:kraft}
  A prefix-free binary code $C : \{1,\dotsc,n\} \to \{0,1\}^*$
  with codeword lengths $\ell_i = \abs{C(i)}$ exists if and only if
  \[ \sum_{i=1}^n \frac{1}{2^{\ell_i}} \leq 1. \]
\end{theorem}
\begin{prf}
  Suppose $C : \{1,\dotsc,n\} \to \{0,1\}^*$ is prefix-free
  with codeword lengths $\ell_i$.
  Let $T$ be its associated binary tree
  and let $W$ be a random walk on $T$ where 0 and 1 have equal weight
  (stopping at either a leaf or undefined branch).

  Define $E_i$ as the event where $W$ reaches $i$ and
  $E_\varnothing$ where $W$ falls off. Then,
  \begin{align*}
    1 & = \Pr(E_\varnothing) + \sum_i \Pr(E_i)                                   \\
      & = \Pr(E_\varnothing) + \sum_i \frac{1}{2^{\ell_i}} \tag{by independence} \\
      & \geq \sum_i \frac{1}{2^{\ell_i}} \tag{probabilities are non-negative}
  \end{align*}

  Conversely, suppose the inequality holds for some $\ell_i$.
  \WLOG, suppose $\ell_1 < \ell_2 < \dotsb < \ell_n$.

  Start with a complete binary tree $T$ of depth $\ell_n$.
  For each $i = 1,\dotsc,n$, find any unassigned node in $T$ of depth $\ell_i$,
  delete its children, and assign it a symbol.

  Now, it remains to show that this process will not fail.
  That is, for any loop step $i$, there is still some unassigned node at depth $\ell_i$.

  Let $P \gets 2^{\ell_n}$ be the number of leaves
  of the complete binary tree of depth $\ell_n$.
  After the $i$\xth step, we decrease $P$ by $2^{\ell_n - \ell_i}$.
  That is, after $n$ steps,
  \begin{align*}
    P & = 2^{\ell_n} - \sum_{i=1}^n \frac{2^{\ell_n}}{2^{\ell_i}}   \\
      & = 2^{\ell_n} - 2^{\ell_n} \sum_{i=1}^n \frac{1}{2^{\ell_i}} \\
      & \geq 0
  \end{align*}
  by the inequality.
\end{prf}

\lecture{May 13}
Recall the problem we are trying to solve:
\bitproblem*
\begin{sol}[Shannon \& Faro]
  Consider the case where $\X$ is symbol $i$ with probability $p_i$.
  We want to encode independent samples $x_i \sim \X$
  as $C(x_i)$ for some code $C : [n] \to \bits*$.

  Suppose for simplification that $p_i = \frac{1}{2^{\ell_i}}$
  for some integers $\ell_i$.
  Since $\sum p_i = 1$, we must have $\sum \frac{1}{2^{\ell_i}} = 1$.
  Then, by \nameref{thm:kraft}, there exists a prefix-free binary code
  $C : [n] \to \bits*$ with codeword lengths $\abs{C(i)} = \ell_i$.
  Now,
  \[
    \E_{x_i \sim \X}\qty[\sum_i\abs{C(x_i)}] = \sum_i p_i\ell_i = \sum_i p_i\log_2\frac{1}{p_i} = H(\X)
  \]
  Proceed to the general case.
  Suppose $\log_2\frac{1}{p_i}$ are non-integral.
  Instead, use $\ell'_i = \ceil*{\log_2\frac{1}{p_i}}$.
  We still satisfy Kraft since $\sum_i \frac{1}{2^{\ell'_i}} \leq \sum_i p_i = 1$.
  Then,
  \[
    \E_{x_i \sim \X}\qty[\sum_i\abs{C(x_i)}] = \sum_i p_i\ell'_i = \sum_i p_i\ceil*{\log_2\frac{1}{p_i}}
  \]
  which is bounded by
  \[ H(\X) = \sum_i p_i\log_2\frac{1}{p_i} \leq \sum_i p_i\ceil*{\log_2\frac{1}{p_i}} < \sum_i p_i\qty(1+\log_2\frac{1}{p_i}) = H(\X) + 1 \]
  We call the code $C$ generated by this process the \term[code!Shannon--Faro]{Shannon--Faro code}.
\end{sol}

We can improve on this bound $[H(\X), H(\X) + 1)$
by amortizing over longer batches of the string.

\begin{sol}[batching]
  For $\Y$ defined on $[n]$ equal to $i$ with probability $q_i$,
  define the random variable $\Y^{(k)}$ on $[n]^k$
  equal to the string $i_1\cdots i_k$ with probability $q_{i_1}\cdots q_{i_k}$.
  That is, $\Y^{(k)}$ models $k$ independent samples of $\Y$.

  Apply the Shannon--Fano code to $\Y^{(k)}$
  to get an encoding of $[n]^k$ as bitstrings of expected length $\ell$
  satisfying $H(\Y^{(k)}) \leq \ell \leq H(\Y^{(k)}) + 1$.
  \begin{align*}
    H(\Y^{(k)}) & = \E_{i_1\cdots i_k \sim \Y^{(k)}}\qty[\log_2 \frac{1}{q_{i_1}\cdots q_{i_k}}] \tag{by def'n}                       \\
                & = \E_{i_1\cdots i_k \sim \Y^{(k)}}\qty[\log_2 \frac{1}{q_{i_1}} + \dotsb + \log_2\frac{1}{q_{i_k}}] \tag{log rules} \\
                & = \sum_{j=1}^k \E_{i_1\cdots i_k \sim \Y^{(k)}}\qty[\log_2 \frac{1}{q_{i_j}}] \tag{linearity of expectation}        \\
                & = \sum_{j=1}^k \E_{i \sim \Y}\qty[\log_2 \frac{1}{q_{i}}] \tag{$q_{i_j}$ only depends on one character}             \\
                & = kH(\Y) \tag{by def'n, no $j$-dependence in sum}
  \end{align*}
  For every $k$ symbols, we use $\ell$ bits, i.e., $\frac{\ell}{k}$ bits per symbol.
  From the Shannon--Faro bound, we have
  \begin{align*}
    \frac{H(\Y^{(k)})}{k} & \leq \frac{\ell}{k} < \frac{H(\Y^{(k)})}{k} + \frac{1}{k} \\
    H(\Y)                 & \leq \frac{\ell}{k} < H(\Y) + \frac{1}{k}
  \end{align*}
  Then, we have a code for $\Y$ bounded by
  $[H(\Y), H(\Y) + \frac{1}{k})$.

  Taking a limit of some sort, we can say that we need $H(\Y) + o(1)$ bits.
\end{sol}

\chapter{Relative entropy}
\begin{defn*}[relative entropy]
  Given two discrete distributions $p = (p_i)$ and $q = (q_i)$,
  the \term[entropy!relative]{relative entropy}
  \[ \D p q :=
    \sum p_i \log_2\frac{1}{q_i} - \sum_i p_i \log_2 \frac{1}{p_i}
    = \sum p_i \log_2 \frac{p_i}{q_i} \]
  This is also known as the \term{KL divergence}.
\end{defn*}

\lecture{May 15}
The KL divergence works vaguely like a ``distance'' between distributions.
(In particular, KL divergence is not a metric since it lacks symmetry
and does not follow the triangle inequality,
but it can act sorta like a generalized squared distance.)

\begin{fact}\label{fact:dnneg}
  $\D p q \geq 0$ with equality exactly when $p = q$.
\end{fact}
\begin{prf}
  % Define $\X' = \frac{p_i}{q_i}$ with probability $p_i$.
  % Then,
  % \[ \D p q = \E[-\log_2 \X'] \geq -\log_2 E[\X'] \]
  % by Jensen's inequality (as $f(x) = -\log_2 x$ is convex), and then
  % \[ \D p q \geq  -\log_2 \sum p_i \frac{q_i}{p_i} = -\log_2 1 = 0 \qedhere \]
  Observe that
  \[ -\D p q = \sum_i p_i(-\log_2\frac{p_i}{q_i}) = \sum_i p_i \log_2 \frac{q_i}{p_i} \]
  and then define $\X' = \frac{q_i}{p_i}$ with probability $p_i$.
  By construction, we get
  \[ -\D p q = \E[\log_2 \X'] \leq \log_2(\E[\X']) \]
  by \nameref{thm:jensen} (as $f = \log_2$ is concave).
  Finally,
  \[ \D p q \geq -\log_2(\E[\X']) = \log_2\qty(\sum_i p_i \frac{q_i}{p_i}) = \log_2 1 = 0  \qedhere \]
\end{prf}

\begin{prop}
  Any prefix-free code has an expected length at least $H(\X)$.
\end{prop}
\begin{prf}
  Let $\X \sim (p_i)$.
  Suppose $C$ is a prefix-free code with codeword lengths $\ell_i$.

  Then, by \nameref{thm:kraft}, $\sum_i 2^{-\ell_i} \leq 1$.
  We want to show that $\sum_i p_i \ell_i \geq H(\X)$,
  and we will prove this by showing that $\sum_i p_i \ell_i - H(\X) =
    \D p q$ for some distribution $q$
  (then apply \cref{fact:dnneg}).

  We will take $q$ to be the random walk distribution corresponding to the binary tree
  associated to the candidate prefix-free code.

  Let $T$ be the binary tree associated to $C$.
  Consider the process of randomly going left/right at each node
  and stopping when either falling off the tree or hitting a leaf.

  Let $q_i = 2^{-\ell_i}$ be the probability that this random walk reaches the leaf for the symbol $i$
  and let $q_{n+1} = 1-\sum_i 2^{-\ell_i}$ be the probability that the random walk falls off the tree.
  Also, to keep ranges identical, let $\tilde p_i = p_i$ and $\tilde p_{n+1} = 0$. Now,
  \begin{align*}
    \D{\tilde p}{q_C}
     & = \sum_{i=1}^{n+1} \tilde p_i \log_2 q_i^{-1} - \sum_{i=1}^{n+1} \tilde p_i \log_2 \frac{1}{p_i}      \\
     & = \sum_{i=1}^n p_i \log_2 2^{\ell_i} - \sum_{i=1}^n p_i \log_2 \frac{1}{p_i} \tag{$\tilde p_{n+1}=0$} \\
     & = \sum_{i=1}^n p_i \ell_i - H(\X)
  \end{align*}
  Therefore, by \cref{fact:dnneg}, $\sum_i p_i \ell_i \geq H(\X)$.
\end{prf}

This proof technique generalizes.
Recall the distinction between UDCs and prefix-free codes:

\defcode*

\begin{example}
  The code $C(1,2,3,4) = (10,00,11,110)$ is a uniquely decodable code.

  The code $C'(1,2,3,4) = (0,10,110,111)$ is a prefix-free code.
\end{example}

\begin{remark}
  A natural additional requirement for unique decodability
  is that for any $k \in \N$, $x \in [n]^k$, $y \in [n]^k$, $C(x) \neq C(y)$.
\end{remark}

\begin{theorem}
  For any uniquely decodable code $C : [n] \to \bits*$ of codeword lengths $\ell_i$,
  there is also a prefix-free code $C' : [n] \to \bits*$ of lengths $\ell_i$.
\end{theorem}

We will show that for any UDC $C$, the lengths $\sum_i 2^{-\ell_i} \leq 1$.
Then, \nameref{thm:kraft} applies and we have a prefix-free code $C'$.

Partition the code's codomain $C([n]) = C_1 \cup C_2 \cup C_3 \cup \cdots$
by the length of the codeword $C_j \subseteq \bits{j}$.
We must instead show $\sum_j \frac{\abs{C_i}}{2^j} \leq 1$.

Consider the easy case $C([n]) = C_2 \cup C_3$.
If there are no collisions of length 5,
we have \[ 2 \cdot \abs{C_2} \cdot \abs{C_3} \leq 2^5 \]
because every string in $\{xy : x \in C_2, y \in C_3\} \cup \{yx : x \in C_2, y \in C_3\}$
is unique in $\bits{5}$.
That is, $\abs{C_2} \cdot \abs{C_3} \leq 2^4$.

Likewise, if there are no collisions of length $5k$, we get
\[ \frac{(2k)!}{k! \cdot k!} \cdot \abs{C_2}^k \cdot \abs{C_3}^k \leq 2^{5k} \]
because the union
$\displaystyle\bigcup_{\mathclap{\substack{\alpha \in \{2,3\}^{2k}, \\ \alpha_i = 2\text{ for} \\ \text{$k$ choices of $i$}}}} C_{\alpha_i}$
consists of only unique strings.

In the limit, by \nameref{thm:sterling},
\begin{align*}
  \frac{2^{2k}}{\sqrt{k}} \cdot \abs{C_2}^k \cdot \abs{C_3}^k & \leq 2^{5k}                                                         \\
  \abs{C_2}\cdot\abs{C_3}                                     & \leq \frac{2^5}{2^2}(\sqrt{k})^{1/k} \approx 1 + \order{\log k / k}
\end{align*}
I have no idea where this was going.

\begin{prf}
  Fix a $k \geq 1$. Let $\ell_{max} = \max \ell_i$.
  Write $C^{(k)}$ to be the set of encoded $k$-length strings.

  Consider the distribution:
  sample a length $m$ uniformly from the set $[k\cdot \ell_{max}]$.
  Also, sample a uniformly random string $s \in \bits{m}$.
  For each $x \in C^{(k)}$, let $E_x$ be the event where $s = x$.

  Now, we can write
  \begin{align*}
    \sum_{x \in C^{(k)}} \Pr[E_x]                                               & \leq 1                \\
    \intertext{because the events $E_x$ are mutually exclusive. Then,}
    \sum_{x \in C^{(k)}} \frac{1}{k\cdot\ell_{max}} \cdot \frac{1}{2^{\ell(x)}} & \leq 1                \\
    \sum_{x \in C^{(k)}} \frac{1}{2^{\ell(x)}}                                  & \leq k\cdot\ell_{max}
  \end{align*}
  where $\ell(x)$ is the length of $x$. Since summing over each codeword $x \in C$
  is the same as summing over each codeword $\ell_i$,
  \begin{align*}
    \qty(\sum_i \frac{1}{2^{\ell_i}})^k
     & = \qty(\sum_{x \in C}\frac{1}{2^{\ell(x)}})^k                                                                      \\
     & = \sum_{x_1,\dotsc,x_k \in C} \frac{1}{2^{\ell(x_1)}} \cdot \frac{1}{2^{\ell(x_2)}} \cdots \frac{1}{2^{\ell(x_k)}} \\
     & = \sum_{x_1,\dotsc,x_k \in C} \frac{1}{2^{\ell(x_1) + \ell(x_2) + \dotsb + \ell(x_k)}}                             \\
     & = \sum_{x_1,\dotsc,x_k \in C} \frac{1}{2^{\ell(x_1x_2 \cdots x_k)}}                                                \\
     & = \sum_{x \in C^{(k)}} \frac{1}{2^{\ell(x)}}
  \end{align*}
  where we can take the last step by uniquely decoding $x_1x_2\cdots x_k$ into $x$.
  Combining,
  \begin{align*}
    \qty(\sum_i \frac{1}{2^{\ell_i}})^k & \leq k \cdot \ell_{max}                            \\
    \sum_i \frac{1}{2^{\ell_i}}         & \leq (k \cdot \ell_{max})^{\frac{1}{k}}            \\
                                        & \leq 1 + \order{\frac{\ell_{max}\cdot\log_2 k}{k}}
  \end{align*}
  which tends to 1 as $k \to \infty$, as desired.
\end{prf}

\lecture{May 20}

\begin{notation}
  Write $H(p)$ to denote $H(\X)$ for $\X \sim \Bern(p)$.

  That is, $H(p) = p\log_2\frac1p + (1-p)\log_2\frac{1}{1-p}$.

  Likewise, write $\D q p$ to be $\D{\Y}{\X}$
  where $\Y \sim \Bern(q)$.
\end{notation}

Recall Sterling's approximation (which we have used before):

\begin{theorem}[Sterling's approximation]\label{thm:sterling}
  $m!$ behaves like $\sqrt{2\pi m}\qty(\frac{m}{e})^m\qty(1+\order{\frac{1}{m}})$
\end{theorem}

\section{The boolean $k$-slice}
Consider the \term{boolean $k$-slice} (also known as the \term{Hamming $k$-slice})
of the hypercube $\bits{n}$ defined by
\[ B_k := \{x \in \bits{n} : \text{$x$ has exactly $k$ ones}\} \]
\begin{remark}
  \[ \abs{B_k} \approx 2^{H(\frac{k}{n})\cdot n} \]
\end{remark}
\begin{prf}
  By \nameref{thm:sterling}, knowing that $\abs{B_k} = \binom{n}{k}$:
  \begin{align*}
    \abs{B_k} & = \binom{n}{k}                                                                                                             \\
              & = \frac{n!}{n!(n-k)!}                                                                                                      \\
              & \approx \frac{\sqrt{2\pi n}\qty(\frac{n}{e})^n}{\sqrt{2\pi k}\qty(\frac{k}{e})^k\sqrt{2\pi(n-k)}\qty(\frac{n-k}{e})^{n-k}} \\
              & = \sqrt{\frac{n}{2\pi k(n-k)}} \cdot \frac{n^k\qty(\frac{n}{n-k})^{n-k}}{k^k}
  \end{align*}
  Now, notice that $\qty(\frac{n}{n-k})^{n-k} = \qty(1+\frac{k}{n-k})^{n-k} \approx e^k$
  for $k \ll n-k$ because $1+x \approx e^x$ for small $x$.
  Then, $\qty(1+\frac{k}{n-k})^{n-k} \approx \qty(e^{k/(n-k)})^{n-k} = e^k$ and
  \begin{align*}
    \abs{B_k} & \approx \qty(\frac{ne}{k})^k \nr{eq:borgor}       \\
              & = 2^{k \log_2 \frac{ne}{k}}                       \\
              & = 2^{k\log_2 \frac{n}{k} + k\log_2 e}             \\
              & = 2^{(\frac{k}{n}\log_2\frac{n}{k})n + k\log_2 e} \\
              & \approx 2^{(\frac{k}{n}\log_2\frac{n}{k})n}
  \end{align*}
  for $1 \ll k \ll n$. Then, given that same assumption,
  \begin{align*}
    H\qty(\frac{k}{n})
     & = \frac{k}{n} \log_2 \frac{n}{k} + \cancelto{0}{\qty(1-\frac{k}{n}) \log_2\frac{1}{1-\frac{k}{n}}} \\
     & \approx \frac{k}{n} \log_2 \frac{n}{k}
  \end{align*}
  because if $n \gg k$, $\frac{k}{n} \to 0$ and $1\log_2 1 = 0$.
  Combining these approximations yields
  \[ \abs{B_k} \approx 2^{H(\frac{k}{n})n} \qedhere \]
\end{prf}

Let $\X$ be a uniformly chosen point in $B_k$
and $\X_1,\dotsc,\X_n \sim \Bern(\frac{k}{n})$.

This means that $H(\X) \approx H((\X_1,\dotsc,\X_n))$,
which is remarkable because the latter could produce points in $B_k$
or points with $n$ ones or points with no ones.

This seems to imply that the majority of the mass of $(\X_1,\dotsc,\X_n)$
lies within the boolean $k$-slice.
Formally, we make the following claim about the
\term{concentration of measure}:\footnote{cf. Dvoretzky--Milman theorem}

\begin{prop}
  Fix any $\varepsilon > 0$. The probability
  \[ \Pr\qty[(\X_1,\dotsc,\X_n) \not\in \bigcup_{\ell = (1-\varepsilon k)}^{(i+\varepsilon)k}B_\ell] = \frac{1}{2^{n/\varepsilon^2}} \]
  Informally, the probability of the randomly-drawn vector lying outside of
  the boolean $k$-slice is exponentially small.
\end{prop}

We will prove a stronger claim:

\begin{claim}
  Fix any $p \in (0,1)$ and consider any $q > p$. Then,
  \[ \Pr[w((\X_i)) > q \cdot n] \leq 2^{-\D q p \cdot n} \]
  where $w((\X_i))$ is the number of ones. Likewise, consider any $q < p$. Then,
  \[ \Pr[w((\X_i)) < q \cdot n] \leq 2^{-\D q p \cdot n} \]
\end{claim}

Consider a toy example first.
Let $\X$ be the number of heads after $n$ fair coin tosses.

Then, $\E[\X] = \frac{n}{2}$ and
\[
  \Pr[\X \geq 0.51n]
  = \frac{1}{2^n}\sum_{k\geq0.51n}^n\binom{n}{k}
  \approx \frac{1}{2^n}\sum_{k\geq0.51n}^n\qty(\frac{ne}{k})^k
  \to 0 \text{ very quickly}
\]
by the same magic that we did in \cref{eq:borgor}
and because $\frac{1}{2^n}$ goes to 0 very quickly.

Now we can prove the claim.

\begin{prf}
  Let $\theta_p(x)$ denote the probability of sampling a vector $x \in \bits{n}$
  where each bit is \iid $\Bern(p)$.
  Then,
  \begin{align*}
    \frac{\theta_p(x)}{\theta_q(x)}
     & = \frac{p^k(1-p)^k}{q^k(1-q)^k}                                            \\
     & = \frac{(1-p)^n}{(1-q)^n}\qty(\frac{\frac{p}{1-p}}{\frac{q}{1-q}})^k       \\
     & \leq \frac{(1-p)^n}{(1-q)^n}\qty(\frac{\frac{p}{1-p}}{\frac{q}{1-q}})^{qn}
  \end{align*}
  for any $k \geq qn$ because (1)
  if $q \geq p$, then $\frac{q}{1-q} \geq \frac{p}{1-p}$
  and the ugly fraction is greater than 1
  and (2) increasing the exponent increases the quantity if the base is greater than 1.

  Let $B_{\geq k} := \bigcup_{\ell \geq k} B_\ell$.
  Then, for all $x \in B_{\geq qn}$, we must show that
  \begin{align*}
    \theta_p(x) \leq \frac{(1-p)^n}{(1-q)^n}\qty(\frac{\frac{p}{1-p}}{\frac{q}{1-q}})^{qn} \cdot \theta_q(x)
    = 2^{-n\D q p\cdot \theta_q(x)}
  \end{align*}
  Consider the right-hand expression:
  \begin{align*}
    2^{n \cdot \D q p}
     & = 2^{n \cdot (q\log_2\frac1p + (1-q)\log_2\frac{1}{1-p} - q\log_2\frac1q - (1-q)\log_2\frac{1}{1-q})} \\
     & = \qty(\frac{1}{p^q} \cdot \frac{1}{(1-p)^{1-q}} \cdot q^q \cdot (1-q)^{1-q})^n                       \\
  \end{align*}
  and the left-hand expression:
  \begin{align*}
    \frac{(1-p)^n}{(1-q)^n}\qty(\frac{\frac{p}{1-p}}{\frac{q}{1-q}})^{qn}
     & = \qty(\frac{(1-p)^{1-q}p^q}{(1-q)^{1-q}q^q})^n                                 \\
     & = \qty(p^q \cdot (1-p)^{1-q} \cdot \frac{1}{q^q} \cdot \frac{1}{(1-q)^{1-q}})^n
  \end{align*}
  which is clearly the reciprocal of the right-hand expression.

  Now, we know that $\theta_p(x) = 2^{-n\D q p}\theta_q(x)$, so
  \begin{align*}
           & \Pr_{\X_1,\dotsc,\X_n \sim \Bern(p)}[(\X_1,\dotsc,\X_n) \in B_{\geq qn}] \\
    ={}    & \sum_{\mathclap{x \in B_{\geq qn}}} \theta_p(x)                          \\
    \leq{} & 2^{-n\D q p}\sum_{\mathclap{x \in B_{\geq qn}}}\theta_q(x)               \\
    \leq{} & 2^{-n\D q p}
  \end{align*}
  since the sum of the probabilities of $x$ being any given entry in $B_{\geq qn}$
  must be at most 1.
\end{prf}

\section{Rejection sampling}

The KL divergence can give us a metric of how accurately we can sample
one distribution using another distribution.

\begin{example}
  Suppose $\X = \begin{cases}
      0 & p=\frac12 \\
      1 & p=\frac12
    \end{cases}$ and $\Y = \begin{cases}
      0 & p=\frac14 \\
      1 & p=\frac34
    \end{cases}$.

  How can we sample $\Y$ using $\X$?
\end{example}
\begin{sol}[naive]
  Take \iid $\X_1$ and $\X_2$.
  Return 0 if $x_1 = x_2 = 0$ and 1 otherwise.
\end{sol}
\begin{sol}[fancy]
  Take an infinite \iid queue $\X_1,\X_2,\dotsc$

  Starting at $i = 1$, if $\X_i = 0$, then output 0 with probability $\frac12$,
  otherwise increment $i$ until $\X_i = 1$.
\end{sol}

\textrule{$\downarrow$ Lecture 6 adapted from Arthur $\downarrow$}

\lecture{May 22}
\begin{problem}[rejection sampling]
  Given access to a distribution $Q = (Q(x))_{x \in \XX}$,
  how efficiently can you simulate $P = (P(x))_{x \in \XX}$?
\end{problem}

\begin{example}
  Suppose $Q = (\frac13,\frac13,\frac13)$ and $P = (\frac12,\frac12)$.
  We want to obtain the $P$ distribution from $Q$.
\end{example}
\begin{sol}
  Since $Q$ and $P$ are both uniform, we can just keep sampling from $Q$
  until we get something in $P$. That is, for $i = 1,\dotsc,\infty$:
  \begin{enumerate}
    \item Sample $\X_i \sim Q$.
    \item If $\X_i \in \{1,2\}$, accept and output $\Y \gets \X_i$.
    \item Otherwise, $i \gets i + 1$.
  \end{enumerate}
  This works because
  \[ \Pr[\Y = 1] = \Pr[\X_i = 1 \mid \X_i = 1 \lor \X_i = 2] = \frac{1/3}{2/3} = \frac12 \]
  for the final round $i$, and similarly for $\Y = 2$.
\end{sol}

\begin{example}
  Consider a slightly more complex distribution $P = (\frac13,\frac23)$
  and $Q = (\frac12,\frac12)$.
\end{example}
\begin{sol}
  We will create a more complex rejection sampling protocol with some cheating.

  Again, iterate and draw independent $\X_i$:
  \begin{itemize}
    \item If $\X_1 = 1$, accept with probability $\frac23$.
          Otherwise, reject and continue to $\X_2$ with probability $\frac13$.
    \item If $\X_1 = 2$, accept.
    \item For $i \geq 2$, accept if $\X_i = 1$ and reject if $\X_i = 2$.
  \end{itemize}
  Then, the probability of accepting $\X_1 = 1$ is $\frac13$,
  $\X_1 = 2$ is $\frac12$, and rejecting $\X_1$ is $\frac16$.

  Since later rounds only output 1,
  we output 1 with probability $\frac13 + \frac16 = \frac12$
  and 2 with probability $\frac12$.
\end{sol}

\begin{defn}[rejection sampler]
  A \term{rejection sampler} is a procedure that reads sequentially independent
  random samples $\X_i \sim Q$ and in each round $i$ either
  \begin{itemize}[nosep]
    \item accepts the value of $\X_i$ and terminates with an index $i^*$, or
    \item rejects and continues.
  \end{itemize}

  The iteration we terminated on $i^*$ is a random variable
  since it is a function of other random variables. It satisfies
  $\X_{i^*} \sim P$, which is weird since for all fixed $i$, $\X_i \sim Q$.
\end{defn}

An interesting application is communication complexity.
Suppose Alice has some hidden distribution $P$.
Alice and Bob have access to a shared random \iid sequence $\X_i \sim Q$.

Alice can send an encoding of $i^*$ to Bob who outputs $\X_{i^*} \sim P$.
This encoding $i^*$ can be encoded using $\log i^*$ bits.

We will show that $\E[\log i^*] \leq \D P Q + \order{1}$.
You can also show that $\D P Q \leq \E[\log i*]$.

For each round $i$ and symbol $x$,
we need to know whether $x$ was sampled before round $i$,
i.e., the probability assigned to $x$ in previous rounds.

For round $i \geq 1$, define:
\begin{itemize}[nosep]
  \item $\alpha_i(x)$ to denote the probability that the procedure accepts $\X_i$ and that $\X_i = x$
  \item $p_i(x)$ to denote the probability that the procedure halts at round $i^* \leq i$ and $\X_{i^*} = x$
\end{itemize}

We want to construct our procedure such that
\begin{itemize}[nosep]
  \item for all $x$, $P(x) = \sum_{i=1}^n \alpha_i(x)$
  \item for all $x$ and $i$, $p_i(x) = \sum_{k=1}^i \alpha_k(x)$
  \item the probability that we halt on or before round $i$ is $p_i^* := \sum_{x \in \XX} p_i(x)$
\end{itemize}

% Then:
% - probability that procedure gets to iteration $i$ and satisfies $X_i=x$ is $(1-p^*_{i-1}) Q(x)$
% 	- $Q(x)$ is the chance that the distribution $Q$ outputs $x$
% 	- this does not necessarily mean that we output it
% - deficit probability of $x$ at round $i$ is
% 	- $P(x) - p_{i-1}(x)$ 
% 		- $P(x)$ is our target
% 		- $p_{i-1}(x)$ is the previous accumulated probability
% - If $(1-p^*_{i-1}) Q(x) > P(x)-p_{i-1}(x)$, 
% 	- then we want $\alpha_i(x) = P(x)-p_{i-1}(x)$ (we never want to exceed the target $P(x)$?)
% 		- this does not prescribe an action
% 	- otherwise we want to always accept, so $\alpha_i(x) = (1-p^*_{i-1}) Q(x)$
% 		- 100% conditioned on getting to round $i$ and sampling $x$
% 	- Therefore: $\alpha_i(x)\coloneqq \min(P(x)-p_{i-1}(x), (1-p_{i-1}^*)Q(x))$
% - 

% Define rejection sampler as:
% ```
% for i = 1 to infty:
% 	Sample X_i ~ Q
% 	If X_i == x:
% 		With probability beta_i(X_i) output i and halt
% 		otherwise continue

% ```

% Define: $\beta_i(X_i) = \frac{\alpha_i(X_i)}{(1-p^*_{i-1})Q(X_i)}$
% - Therefore, the bias of the coin (aka this beta_i x_i) changes on each round
% 	- but we only flip a coin once

\textrule{$\uparrow$ Lecture 6 adapted from Arthur $\uparrow$}

\lecture{May 27}
\begin{algorithm}[H]
  \caption{\Call{RejectionSampling}{$P$, $Q$}}
  \begin{algorithmic}[1]
    \Require{$\forall x \in \XX, Q(x) > 0 \iff \D P Q < \infty$}
    \For{$x \in \XX$} $p_0(x) \gets 0$ \EndFor
    \State $p_0^* \gets 0$
    \For{$i = 1,\dotsc,\infty$}
      \State sample $\X_i \sim Q$
      \If{$P(\X_i) - P_{i-1}(\X_i) \leq (1-p^*_{i-1})\cdot Q(\X_i)$}
        \Prob{$\beta_i(\X_i) = \frac{P(\X_i) - p_{i-1}(\X_i)}{(1-p_{i-1}^*)(Q(\X_i))}$}
          \LComment{so that the net probability of sampling $\X_i$ will be $\alpha_i(\X_i) = P(\X_i) - p_{i-1}(\X_i)$}
          \State \Return $\X_i$
        \EndProb
      \Else{}
        \Prob{$\beta_i(\X_i) = 1$}
          \LComment{so that the net probability of sampling $\X_i$ is $\alpha_i(1-p_{i-1}^*)\cdot Q(\X_i)$}
          \State \Return $\X_i$
        \EndProb
      \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

In this case, for all $x$ and for all $i$:
\begin{itemize}[nosep]
  \item the probability of accepting $x$ in round $i$
        is $\alpha_i(x) = \min\{P(x) - p_{i-1}(x), (1-p_{i-1}^*)Q(x)\}$
  \item the probability of accepting $x$ on or before round $i$ is
        $p_i(x) = p_{i-1}(x) + \alpha_i(x)$
  \item the probability of terminating on or before round $i$ is
        $p_i^* = p_{i-1}^* + \sum_{x \in \XX} \alpha_i(x) = \sum_{x \in \XX} p_i(x)$
\end{itemize}

\begin{example}
  Let $P = (\frac12,\frac38,\frac18)$ and $Q = (\frac13,\frac13,\frac13)$.
  Do the procedure.
\end{example}
\begin{sol}
  In round 1, sample $\X_1 \sim Q$.
  \begin{itemize}[nosep]
    \item If $\X_1 = 1$, accept with probability 1.
    \item If $\X_1 = 2$, accept with probability 1.
    \item If $\X_1 = 3$, accept with probability $\frac38$.
  \end{itemize}
  Then, $p_1(1) = \frac13$, $p_1(2) = \frac13$, $p_1(3) = \frac18$, and $p_1^* = \frac{19}{24}$.

  In round 2, sample $\X_2 \sim Q$.
  \begin{itemize}[nosep]
    \item If $\X_2 = 1$, accept with probability 1.
          There is a $\frac{5}{72}$ chance of getting here,
          but deficit probability is $\frac16$, so no need to reduce.
    \item If $\X_2 = 2$, accept with probability $\frac35$.
          There is a $\frac{5}{72}$ chance of getting here and
          deficit probability is $\frac38 - \frac13 = \frac{1}{24}$.
          For equality, use probability $\frac{3}{5}\cdot\frac{5}{72} = \frac{1}{24}$.
    \item If $\X_3 = 3$, accept with probability 0.
          We already fulfilled $P(3) = p_1(3)$.
  \end{itemize}
  Then, $p_2(1) = \frac{29}{72}$, $p_2(2) = \frac38$, $p_3(2) = \frac18$,
  and $p_2^* = \frac{19}{24} + \frac{5}{24}\cdot(\frac13+\frac{3/5}{5}) = \frac{65}{72}$.

  In round 3, sample $\X_3 \sim Q$.
  \begin{itemize}[nosep]
    \item If $\X_3 = 1$, accept with probability 1.
    \item If $\X_3 = 2$ or 3, accept with probability 0.
  \end{itemize}
  Keep repeating until we accept a 1.
\end{sol}

\begin{prop}
  $(p_i(x))_{x \in \XX}$ converges to $P(x)$ as $i \to \infty$.
  In fact, the residual decays exponentially fast
  \[ P(x) - p_i(x) \leq P(x) \cdot (1-Q(x))^i. \]
\end{prop}
\begin{prf}
  Begin with the claim that the probability of reaching round $i$
  is at least the residual at $i$ for any $x$:
  \[ 1-p_{i-1}^* \geq P(x) - p_{i-1}(x) \quad \forall x \]
  Intuitively, either you returned prior to round $i$ (i.e., $p_{i-1}^*$)
  or you did not (i.e., the residual).
  \begin{align*}
    1 - p_{i-1}^*
     & = \sum_{x \in \XX} P(x) - \sum_{x \in \XX} p_{i-1}(x)    \\
     & = \sum_{x \in \XX} (P(x) - p_{i-1}(x)) \nr{eq:subclaim1}
  \end{align*}
  Also, claim that
  \[ \alpha_i \geq (P(x) - p_{i-1}(x))\cdot Q(x) \nr{eq:subclaim2} \]
  If $\alpha_i = P(x) - p_{i-1}(x)$, then clearly $\alpha_i \geq \alpha_i Q(x)$.
  Otherwise, if $\alpha_i = (1-p_{i-1}^*)Q(x)$, then \cref{eq:subclaim1} applies.

  Proceed by induction.

  Base case: exercise.

  Inductive step: suppose that
  $P(x) - p_i(x) \leq P(x) \cdot (1-Q(x))^i$. Then,
  \begin{align*}
    P(x) - p_{i+1}(x)
     & = P(x) - p_i(x) - \alpha_{i+1}(x)                             \\
     & \leq (P(x) - p_{i-1}(x))(1-Q(x)) \tag{by \cref{eq:subclaim2}} \\
     & \leq (P(x) \cdot (1-Q(x))^i)(1-Q(x)) \tag{by supposition}     \\
     & \leq P(x) \cdot (1-Q(x))^{i+1} \qedhere
  \end{align*}
\end{prf}

Now, we will prove that this is related to relative entropy.

\begin{prop}\label{prop:rejworst}
  Let $i^*$ be the iteration at which the procedure returns.
  Then, $\E[\log_2 i^*] \leq \D P Q + 2\log_2 e$.
\end{prop}
\begin{prf}
  First, claim that for all $x \in \XX$ and any $i \geq 2$
  such that $\alpha_i(x) > 0$,
  \[ i \leq \frac{P(x)}{(1-p_{i-1}^*) \cdot Q(x)} + 1 \nr{eq:maxi} \]
  That is, if we reach a particular round $i$,
  the probability mass left must be sufficiently large.

  We know that $P(x) \geq p_{i-1}(x)$ since we increase to $P(x)$. Then,
  \begin{align*}
    P(x) & \geq p_{i-1}(x)                                               \\
         & = \alpha_1(x) + \dotsb + \alpha_{i-1}(x)                      \\
         & \geq (1-p_1^*)\cdot Q(x) + \dotsb + (1-p_{i-1})\cdot Q(x)     \\
         & \geq (1-p_{i-1}^*)\cdot Q(x) + \dotsb + (1-p_{i-1})\cdot Q(x) \\
         & = (i-1)(1-p_{i-1}^*)\cdot  Q(x)                               \\
    i    & \leq \frac{P(x)}{(1-p_{i-1}^*) \cdot Q(x)} + 1
  \end{align*}
  as long as $\alpha_{j-1} < \alpha_j$ for all $j$.

  \lecture{May 29}
  Do a gigantic algebra bash:
  \begin{align*}
    \E[\log_2 i^*]
     & = \sum_{i=1}^\infty (p_i^* - p_{i-1}^*) \cdot \log_2 i                                                                                                                                     \\
     & = \sum_{i=1}^\infty \sum_{x\in\XX} \alpha_i(x) \cdot \log_2 i                                                                                                                              \\
     & \leq \sum_{i=1}^\infty \sum_{x\in\XX} \alpha_i(x) \cdot \log_2 \qty[\frac{P(x)}{(1-p_{i-1}^*) Q(x)}+1] \tag{by \cref{eq:maxi}}                                                             \\
     & \leq \sum_{i=1}^\infty \sum_{x\in\XX} \alpha_i(x) \cdot \log_2 \qty[\frac{1}{(1-p_{i-1}^*)}\qty(\frac{P(x)}{Q(x)}+1)]                                                                      \\
     & = \underbrace{\sum_{i=1}^\infty \sum_{x\in\XX} \alpha_i(x) \log_2 \frac{1}{(1-p_{i-1}^*)}}_A + \underbrace{\sum_{i=1}^\infty \sum_{x\in\XX} \alpha_i(x) \log_2\qty(\frac{P(x)}{Q(x)}+1)}_B
  \end{align*}
  Consider the first term $A$:
  \begin{align*}
    A & = \sum_{i=1}^\infty \sum_{x\in\XX}\alpha_i(x) \log_2 \frac{1}{(1-p_{i-1}^*)} \\
      & = \sum_{i=1}^\infty (p_i^*-p_{i-1}^*) \log_2\frac{1}{(1-p_{i-1}^*)}
    \intertext{Notice that this is a left-handed Riemann sum of $\log_2\frac{1}{1-x}$:}
    A & \leq \int_0^1 \log_2\frac{1}{1-x} \dd{x}                                     \\
      & = \log_2 e
  \end{align*}
  Now, consider the second term $B$:
  \begin{align*}
    B & = \sum_{i=1}^\infty \sum_{x\in\XX} \alpha_i(x) \log_2\qty(\frac{P(x)}{Q(x)}+1)                            \\
      & = \sum_{x\in\XX} \sum_{i=1}^\infty  \alpha_i(x) \log_2\qty(\frac{P(x)}{Q(x)}+1) \tag{Fubini?}             \\
      & = \sum_{x\in\XX} P(x) \log_2\qty(\frac{P(x)}{Q(x)}+1)\tag{$P(x)=\sum_i \alpha_i(x)$}                      \\
      & = \sum_{x\in\XX} P(x) \log_2\qty(\frac{P(x)}{Q(x)}\cdot\qty(1+\frac{Q(x)}{P(x)}))                         \\
      & = \sum_{x\in\XX} P(x) \log_2\qty(\frac{P(x)}{Q(x)}) + \sum_{x\in\XX} P(x) \log_2\qty(1+\frac{Q(x)}{P(x)}) \\
      & = \D P Q + \sum_{x\in\XX} P(x) \log_2\qty(1+\frac{Q(x)}{P(x)})                                            \\
      & \leq \D P Q + \sum_{x\in\XX} P(x) \log_2\qty(e^{Q(x)/P(x)}) \tag{$1+x\leq e^x$ for all $x \geq 0$}        \\
      & = \D P Q + \sum_{x\in\XX} P(x) \frac{Q(x)}{P(x)}\log_2 e                                                  \\
      & = \D P Q + \log_2 e\sum_{x\in\XX} Q(x)                                                                    \\
      & = \D P Q + \log_2 e
  \end{align*}
  Therefore,
  \[ \E[\log_2 i^*] \leq A + B \leq \D P Q + 2\log_2 e \]
  completing the proof.
\end{prf}

Intuition: for any $x\in\XX$, if $\alpha_i(x)\leq Q(x) \lll P(x)$,
then you need an expected amount of $\frac{P(x)}{Q(x)}$ steps to succeed,
because you just won't roll $x$ that often.

Also, if $\alpha_{i+1}(x)>0$ (any round prior to termination),
$(1-p_{i-1}^*(x))Q(x) \leq \alpha_i(x)$.

\begin{prop}\label{prop:rejbest}
  For any rejection sampler, let $i^*$ be the index where it returns. Then,
  \[ \E[\ell(i^*)] \geq \D P Q \]
\end{prop}
\begin{prf}
  For convenience, redefine $\alpha_i(x) \coloneqq \Pr[i^* = i \land \X_i = x]$.

  First, observe that for any $x\in\XX$, a rejection sampler must have
  \[ \alpha_i(x) \leq Q(x) \]
  because we only have a $Q(x)$ chance of rolling $x$ to accept it in round $i$.

  Now, fix $x\in\XX$. Consider the random variable $i^*|_{\X_{i^*}=x}$.
  Then, by \nameref{thm:kraft},
  \begin{align*}
    \E[\ell(i^*) \mid \X_{i^*}=x]
     & \geq H(i^* \mid \X_{i^*}=x)                                                             \\
     & = \sum_{i=1}^\infty \Pr[i^*=i\mid \X_{i^*}=x]\log_2\frac{1}{\Pr[i^*=i \mid \X_{i^*}=x]} \\
     & = \sum_{i=1}^\infty \frac{\alpha_i(x)}{P(x)}\log_2\frac{P(x)}{\alpha_i(x)}              \\
     & \geq \sum_{i=1}^\infty \frac{\alpha_i(x)}{P(x)}\log_2\frac{P(x)}{Q(x)}                  \\
     & = \log_2\frac{P(x)}{Q(x)} \cdot \sum_{i=1}^\infty \frac{\alpha_i(x)}{P(x)}              \\
     & = \log_2\frac{P(x)}{Q(x)}
  \end{align*}
  because $\sum_{i=1}^\infty \alpha_i(x) = P(x)$.
  Apply the law of total probability:
  \begin{align*}
    \E[\ell(i^*)] & = \sum_{x\in \XX} \Pr[\X_{i*}=x] \E[\ell(i^*) \mid \X_{i^*}=x] \\
                  & = \sum_{x\in \XX} P(x) \E[\ell(i^*) \mid \X_{i^*}=x]           \\
                  & \geq \sum_{x\in \XX} P(x) \log_2 \frac{P(x)}{Q(x)}             \\
                  & = \D P Q
  \end{align*}
  as desired.
\end{prf}

\chapter{Mutual information}

\section{Definition and chain rules}
\lecture{June 3}
\begin{notation}
  Given two jointly distributed random variables $(\X,\Y)$ over sample space
  $\XX \times \YY$, write $p_{xy}$ for $\Pr[\X = x, \Y = y]$.
\end{notation}
\begin{defn}
  Given two jointly distributed random variables $(\X,\Y)$ over sample space
  $\XX \times \YY$, define the \term{mutual information} $I(\X : \Y)$ by
  \begin{align*}
    I(\X : \Y) & = H(\X) + H(\Y) - H((\X,\Y)) \\
               & = H(\X) - H(\X \mid \Y)      \\
               & = H(\Y) - H(\Y \mid \X)
  \end{align*}
  where the \term[entropy!conditional]{conditional entropy} $H(\X \mid \Y)$ is
  \[ \sum_{y \in \YY} p_y \cdot H((\X|_{\Y=y})) \]
\end{defn}
This is entirely analogous to saying that
$\abs{A \cap B} = \abs{A} + \abs{B} - \abs{A \cup B} = \abs{A} - \abs{A \setminus B}$.

\begin{theorem}[chain rule for entropy]\label{thm:chain}
  Given two jointly distributed random variables $(\X,\Y)$
  over a discrete sample space $\XX \times \YY$,
  \[ H((\X,\Y)) = H(\X) + H(\Y \mid \X) \]
\end{theorem}
\begin{prf}
  Do a bunch of algebra:
  \begin{align*}
    H(\X) + H(\Y\mid\X)
     & = \sum_{x\in\XX}p_x\log\frac{1}{p_x} + \sum_{x\in\XX}p_x \sum_{y\in\YY} \Pr[\Y=y\mid\X=x]\log\frac{1}{\Pr[\Y=y\mid\X=x]}           \\
     & = \sum_{x\in\XX}p_x\log\frac{1}{p_x} + \sum_{x\in\XX}\cancel{p_x} \sum_{y\in\YY} \frac{p_{xy}}{\cancel{p_x}}\log\frac{p_x}{p_{xy}} \\
     & = \sum_{\substack{x\in\XX\br y\in\YY}}p_{xy}\log\frac{1}{p_x} + \sum_{\substack{x\in\XX\br y\in\YY}} p_{xy}\log\frac{p_x}{p_{xy}}  \\
     & = \sum_{\substack{x\in\XX\br y\in\YY}}p_{xy}\qty(\log\frac{1}{p_x} + \log\frac{p_x}{p_{xy}})                                       \\
     & = \sum_{\substack{x\in\XX\br y\in\YY}}p_{xy}\log\frac{1}{p_{xy}}                                                                   \\
     & = H((\X,\Y)) \qedhere
  \end{align*}
\end{prf}
\begin{corollary}
  For two independent variables, since $(\Y \mid \X) = \Y$,
  we have $H((\X,\Y)) = H(\X) + H(\Y)$ as expected.
\end{corollary}
\begin{corollary}
  $H((\X_1,\X_2,\X_3)) = H(\X_1) + H(\X_2 \mid \X_1) + H(\X_3 \mid (\X_1,\X_2))$
\end{corollary}
\begin{prf}
  Consider $(\X_1,\X_2,\X_3) = ((\X_1,\X_2),\X_3)$.
  Then, by the \nameref{thm:chain},
  \[ H(((\X_1,\X_2),\X_3)) = H((\X_1,\X_2)) + H(\X_3 \mid (\X_1,\X_2)) \]
  and then by another application,
  \[ H(((\X_1,\X_2),\X_3)) = H(\X_1) + H(\X_2 \mid \X_1) + H(\X_3 \mid (\X_1,\X_2)) \]
  as desired.
\end{prf}
\begin{theorem}[general chain rule for entropy]
  For $k$ random variables $\X_1,\dotsc,\X_k$,
  \[ H((\X_1,\dotsc,\X_k)) = \sum_{i=1}^k H(\X_i \mid (\X_1,\cdots,\X_{i-1})) \]
\end{theorem}
\begin{prf}
  By induction on the \nameref{thm:chain}.
\end{prf}

\begin{notation}
  Although relative entropy is defined only on \emph{distributions},
  write $\D\X\Y$ to be $\D{f_{\X}}{f_{\Y}}$
  where $\X \sim f_{\X}$ and $\Y \sim f_{\Y}$.
\end{notation}

\begin{theorem}[chain rule for relative entropy]\label{thm:chainD}
  Let $p$ and $q : \XX\times\YY \to [0,1]$ be distributions.
  Let $p(x) \coloneqq \sum_{y\in\YY} p(x,y)$ denote marginals of $p$
  and $p(y|x) \coloneqq \frac{p(x,y)}{p(x)}$ denote conditionals of $p$.
  Then,
  \begin{align*}
    \D{p(x,y)}{q(x,y)}
     & = \D{p(x)}{q(x)} + \D{p(y|x)}{q(y|x)}                                                   \\
     & = \D{p(x)}{q(x)} + \sum_{x\in\XX} p(x) \cdot \D{(p(y|x))_{y\in\YY}}{(q(y|x))_{y\in\YY}}
  \end{align*}
  where $\D{p(y|x)}{q(y|x)}$
  is the \term[entropy!relative!conditional]{conditional relative entropy}.

  Equivalently, let $(\X_1,\Y_1)$ and $(\X_2,\Y_2)$ be two joint random variables.
  Then,
  \[
    \D{(\X_1,\Y_1)}{(\X_2,\Y_2)}
    = \D{\X_1}{\X_2} + \sum_{x\in\XX}\Pr[\X_1=x]\cdot\D{\Y_1|_{\X_1=x}}{\Y_2|_{\X_2=x}}
  \]
\end{theorem}
\begin{prf}[for distributions]
  Do algebra:
  \begin{align*}
        & \D{p(x)}{q(x)} + \D{p(y \mid x)}{q(y \mid x)}                                                                                                \\
    ={} & \sum_{x\in\XX} p_x \log \frac{p_x}{q_x} + \sum_{x\in\XX}p_x\sum_{y\in\YY}p(y\mid x) \log\frac{p(y\mid x)}{q(y\mid x)}                        \\
    ={} & \sum_{x\in\XX} p_x \log \frac{p_x}{q_x} + \sum_{x\in\XX}p_x\sum_{y\in\YY}\frac{p_{xy}}{p_x} \log\frac{p_{xy}q_x}{q_{xy}p_x}                  \\
    ={} & \sum_{\substack{x\in\XX\br y\in\YY}} p_{xy} \log \frac{p_x}{q_x} + \sum_{\substack{x\in\XX\br y\in\YY}}p_{xy}\log\frac{p_{xy}q_x}{q_{xy}p_x} \\
    ={} & \sum_{\substack{x\in\XX\br y\in\YY}} p_{xy} \qty(\log \frac{p_x}{q_x} + \log\frac{p_{xy}q_x}{q_{xy}p_x})                                     \\
    ={} & \sum_{\substack{x\in\XX\br y\in\YY}} p_{xy} \log\frac{p_{xy}}{q_{xy}}                                                                        \\
    ={} & \D{p(x,y)}{q(x,y)}
  \end{align*}
  as in the proof of \nameref{thm:chain}.
\end{prf}
\begin{fact}
  \[ I[\X : \Y] = \E_{x \gets \X}[\D{\Y|_{\X=x}}{\Y}] = \sum_{x\in\XX}p_x\D{\Y|_{\X=x}}{\Y} \]
\end{fact}
\begin{prf}
  First, claim that
  \[
    I[\X : \Y] = \D{(\X,\Y)}{\tilde\X \otimes \tilde\Y}
    \nr{eq:claim1}
  \]
  where $\tilde\X \otimes \tilde\Y$ denotes a random variable consisting of
  $\tilde\X$ (resp.\ $\tilde\Y$) independently sampled
  according to the distribution of $\X$ (resp.\ $\Y$)
  so that $\Pr[\tilde\X=x,\tilde\Y=y]=p_x p_y$. Expand the left-hand side:
  \begin{align*}
    I[\X : \Y]
     & = \sum_{x\in\XX} p_x \log\frac1{p_x} + \sum_{y\in\YY} p_y \log\frac1{p_y} - \sum_{\substack{x\in\XX\br y\in\YY}}p_{xy}\log\frac1{p_{xy}}                   \\
     & = \sum_{x\in\XX}\sum_{y\in\YY} p_{xy} \log\frac1{p_x} + \sum_y\sum_x p_{xy} \log\frac1{p_y} - \sum_{\substack{x\in\XX\br y\in\YY}}p_{xy}\log\frac1{p_{xy}} \\
     & = \sum_{x\in\XX}\sum_{y\in\YY} p_{xy} \qty(\log\frac1{p_x} + \log\frac1{p_y} - \log\frac1{p_{xy}})                                                         \\
     & = \sum_{\substack{x\in\XX\br y\in\YY}} p_{xy} \log\frac{p_{xy}}{p_x p_y}                                                                                   \\
     & = \D{(\X,\Y)}{\tilde\X \otimes \tilde\Y}
  \end{align*}
  Now, apply the \nameref{thm:chainD}:
  \begin{align*}
    \D{(\X,\Y)}{\tilde\X \otimes \tilde\Y}
     & = \D{\X}{\tilde\X} + \D{(\X,\Y)\mid(\X,\tilde\X)}{(\tilde\X\oplus\tilde\Y)\mid(\X,\tilde\X)} \\
     & = 0 + \sum_x p_x \D{\Y|_{\X=x}}{\Y}                                                          \\
     & = \E_{x\gets\X}\D{\Y|_{\X=x}}{\Y} \qedhere
  \end{align*}
\end{prf}

\skipto[lecture]{11}
\lecture{June 10}
\begin{theorem}[chain rule for mutual information]
  Let $\X_1$, $\X_2$, and $\Y$ be random variables. Then,
  \[ I((\X_1,\X_2) : \Y) = I(\X_1 : Y) + I(\X_2 : (Y \mid \X_1)) \]
  and in general
  \[ I((\X_1,\dotsc,\X_n) : \Y) = \sum_{i=1}^n I(\X_1 : (\Y \mid (\X_1,\dotsc,\X_{i-1}))) \]
\end{theorem}

\section{Markov chains and data processing}
\begin{defn}
  The random variables $\X$, $\Y$, and $\rv Z$ form a \term{Markov chain}
  if the conditional distribution of $\rv Z$ depends only on $\Y$
  and is conditionally independent of $\X$. Equivalently,
  \[ \Pr[\X=x,\Y=y,\rv Z=z] = \Pr[\X=x]\cdot\Pr[\Y=y\mid\X=x]\cdot\Pr[\rv Z=z\mid\Y=y] \]
  Then, we write $\X \to \Y \to \rv Z$.
\end{defn}

\begin{example}[\textit{\href{https://en.wikipedia.org/wiki/Drunken_Master_II}{Legend of the Drunken Master}}]
  In $\Omega = \R^2$, Jackie Chan is drunk and takes steps in random directions.
  He starts at $\rv J_0 = (0,0)$.
  Then, $\rv J_1 = \rv J_0 + d_1$ where $d_1$ is an independent random unit vector in $\R^2$,
  and $\rv J_2 = \rv J_1 + d_2$ and so on.
\end{example}

First, $\rv J_3$ and $\rv J_1$ are not independent.
But if we fix $\rv J_2 = j_2 \in \R^2$, then $\rv J_1 \mid \rv J_2=j_2$
and $\rv J_3 \mid \rv J_2=j_2$ are independent.
In fact, they are uniformly distributed random points on the circle of radius 1
centred at $j_2$.

\begin{prop}[Markov chain characterization]\label{prop:markov}
  Let $\X$, $\Y$, and $\rv Z$ be random variables. \TFAE:
  \begin{enumerate}
    \item $\X \to \Y \to \rv Z$
    \item $\X$ and $\rv Z$ are conditionally independent given $\Y$.
          That is,
          \[
            \Pr[\X=x,\rv Z=z \mid \Y=y] = \Pr[\X=x \mid \Y=y] \cdot \Pr[\rv Z=z \mid \Y=y]
          \]
    \item $\rv Z$ is distributed according to $f(\Y,\rv R)$ for some $\rv R$ independent of $\X$ and $\Y$.
  \end{enumerate}
\end{prop}
\begin{xca}
  Prove the definitions are equivalent.
\end{xca}

\begin{theorem}[data-processing inequality]\label{thm:dpi}
  If $\X \to \Y \to \rv Z$, then $I(\X : \rv Z) \leq I(\X : \Y)$.

  Equality happens if and only if $\X \to \rv Z \to \Y$.
\end{theorem}
\begin{prf}
  By the chain rule,
  \[
    I(\X : (\Y, \rv Z)) = I(\X : \Y) + \cancelto{0}{I(\X : \rv Z \mid \Y)}
    = I(\X : \rv Z) + I(\X : \Y \mid \rv Z)
  \]
  so that
  \[ I(\X : \Y) = I(\X : \rv Z) + I(\X : \Y \mid \rv Z) \]
  One may show that the mutual information is always non-negative,
  so we have $I(\X : \Y) \geq I(\X : \rv Z)$ as desired.
  We defer the proof of the equality case for \cref{sec:ss}.
\end{prf}

\section{Communication complexity}
\skipto[lecture]{10}
\lecture{June 5}
\begin{problem}
  Suppose there is a joint distribution $(\X,\Y)$ that Alice and Bob wish to
  jointly compute. Alice and Bob have access to a shared random string $\rv R = (\rv R_i)$.
  Alice is given $x\in\XX$ and wants to send Bob a prefix-free message of minimum length
  so that Bob can compute a sample from $\Y \mid \X=x$.
\end{problem}

\begin{defn}
  A \term*{protocol} $\Pi$ is a pair of functions $(M,y)$ where
  $M : \XX\times\Omega_{\rv R} \to \bits*$ is the message Alice sends to Bob
  and $y : \bits*\times\Omega_{\rv R} \to \YY$ is Bob's output.

  The \term*{performance} of $\Pi$ is $\E_{\X,\rv R}\abs{M(\rv X,\rv R)}$
\end{defn}

Suppose $\X$ and $\Y$ are independent.
Then, Bob needs no information so we can use the trivial protocol
$M(\X,\rv R) = \varnothing$ with performance 0.

Otherwise, we can use a strategy of prefix-free encoding $x$
so that $\E\abs{M(\X,\rv R)} \approx H(\X)$.

\begin{theorem}
  There exists a protocol $\Pi = (M,y)$ such that expected message length
  \[ \E\abs{M(\X,\rv R)} \leq I(\X : \Y) + \order{\log I(\X : \Y)} \]
  For all other protocols $\Pi' = (M',y')$,
  \[ \E\abs{M'(\X,\rv R)} \geq I(\X : \Y) \]
\end{theorem}
\begin{prf}
  Let $\X$ be a random point on the hypercube $\{\pm1\}^n$.
  Let $\Y$ be a random point on $\{\pm1\}^n$ that is $\varepsilon$-correlated with $\X$.
  That is, $\Y_i = \X_i$ with probability $\varepsilon$
  and is uniformly random otherwise.

  Observe that, individually, $\X$ and $\Y$ have the same distribution.
  In particular, in the $\varepsilon$ case, then $\Y_i = \X_i$ is $\Unif\{\pm1\}$.
  In the $1-\varepsilon$ case, $\Y_i \sim \Unif\{\pm1\}$ by definition.

  We can calculate $H(\X) = H(\Y) = n$.

  Also, $H(\Y\mid\X) = \sum_{x} p_x H(\Y\mid\X=x) \approx (1-\varepsilon)n$.
  One can show that $\Y\mid\X=x$ is approximately uniformly distributed
  over the vectors of length $n$ that agree on $\varepsilon n$ coordinates with $x$.
  This sample space has size $2^{(1-\varepsilon)n}$.

  Therefore, $I(\X:\Y) = H(\Y) - H(\Y \mid \X) \approx \varepsilon n$.

  By \cref{prop:rejworst}, there exists a rejection sampler such that
  $\E[\ell(i^*)] \leq \D P Q + \order{\log \D P Q}$.

  Recall from STAT 230 that we can transform $\rv R$ into any distribution
  with the change of variable bullshit.
  In particular, transform $\rv R_i$ to \iid $\Y_i \sim \Y$ and the biased coins.

  Alice will run \Call{RejectionSampler}{$\Y|_{\X=x},\Y$} to find a random index $i^*$
  such that $\Y_{i^*}$ has distribution $\Y|_{\X=x}$.

  Alice sends a prefix-free encoding of $i^*$. Bob outputs $\Y_{i^*}$.
  The performance is:
  \begin{align*}
    \E_{\mathclap{\X,\rv R}} \abs{M(\X,\rv R)}
     & = \sum_{x\in\XX} p_x \E_{i^*,\rv Y_1,\rv Y_2,\dotsc}[\ell(i^*)]                    \\
     & \leq \sum_{x\in\XX} p_x \qty(\D{\Y|_{\X=x}}{\Y} + \order{\log \D{\Y|_{\X=x}}{\Y}}) \\
     & = I(\X:\Y) + \sum_{x\in\XX}p_x\order{\log \D{\Y|_{\X=x}}{\Y}}                      \\
     & \leq I(\X:\Y) + \order{\log I(\X:\Y)}
  \end{align*}
  where the last step is by Jensen's inequality.

  Now, let $\Pi$ be any protocol.
  \marginnote{\normalsize{\textit{Lecture 11\\June 10\\(con'd)}}}
  We will apply the \nameref{thm:dpi}.

  Notice that $\X \to (M(\X,\rv R),\rv R) \to \Y$ if and only if $\Pi$
  is a valid protocol.
  If we sample $x\sim\X$ and Alice sends $M(x,\rv R)$,
  then Bob outputs something distributed according to $\Y \mid \X=x$,
  i.e., just $\Y$ since $x$ was arbitrary. Then,
  \begin{align*}
    I(\X : \Y)
     & \leq I(\X : (M(\X,\rv R),\rv R)) \tag{data processing inequality}                                          \\
     & = I(\X : \rv R) + \sum_{r\in\Omega_{\rv R}} p_r I(\X|_{\rv R=r} : M(\X,\rv R)|_{\rv R=r}) \tag{chain rule} \\
     & = 0 + I(\X : M(\X,\rv R) \mid \rv R) \tag{independence}                                                    \\
     & \leq H(M(\X,\rv R) \mid \rv R) \tag{$I(\rv A : \rv B) \leq \min\{H(\rv A),H(\rv B)\}$}                     \\
     & \leq H(M(\X,\rv R)) \tag{$H(\rv A \mid \rv B) \leq H(\rv A)$}                                              \\
     & \leq \E\abs{M(\X,\rv R)} \tag{Kraft inequality}
  \end{align*}
  completing the proof.
\end{prf}

\section{Sufficient statistics}\label{sec:ss}
\skipto[lecture]{12}
\lecture{June 12}
We will develop the idea of sufficient statistics and data processing
towards the asymptotic equipartition property.
This is a warmup for the joint asymptotic equipartition property
which we will use to prove one direction of Shannon's channel-coding theorem.

\begin{problem}
  Suppose $\X = (\X_1,\dotsc,\X_n)$ are \iid sampled according to $\Bern(\theta)$
  for some fixed parameter $\theta \in [0,1]$.

  If we have a sample $x = (x_1,\dotsc,x_n)$, how can we recover $\theta$?
\end{problem}

The classical solution (recall from STAT 230) is the maximum likelihood estimator
$\hat\theta = \frac1n\sum_{i=1}^n x_i$ such that
$\Pr[\abs{\hat\theta-\theta} > \varepsilon] \leq 2^{-\Omega(\varepsilon^2n)}$.
In essence, we are reducing the number of bits to send $\theta$ from $n$
to [whatever it is you need to send a float of desired accuracy lol].

\begin{defn}
  A function $T(\X)$ is a \term{sufficient statistic}
  relative to a family $\{f_\theta(x)\}$ if $\theta \to T(\X) \to \X$.
\end{defn}

We are considering the case where $f_\theta$ is $\Bern(\theta)$.
Clearly, $\theta \to \X \to T(\X)$ is a Markov chain
because $\X$ is distributed based on $\theta$ and $T$ is a function of
$\X$ which is not influenced $\theta$.

\begin{example}
  $T(\X) = \frac1n\sum_{i=1}^n \X_i$ is a sufficient statistic
  relative to the family $\{\Bern(\theta)\}$.
\end{example}
\begin{prf}
  We must show $\theta \to T(\X) \to \X$ is a Markov chain.

  Fix $x = (x_1,\dotsc,x_n)$. Notice that
  \[ \Pr\qty[\X_1=0,\dotsc,\X_n=0 \mid \frac1n\sum \X_i = \frac12] = 0 \]
  and
  \[ \Pr\qty[\X_1=1,\dotsc,\X_n=1 \mid \frac1n\sum \X_i = \frac12] = 0 \]
  since we obviously cannot have half the $\X_i$'s be 1 if they are all 0s or all 1s.

  But if we set exactly half of the $\X_i$'s to be 1, the distribution is uniform
  \[ \Pr\qty[\X_1=1,\dotsc,\X_{\frac{n}{2}}=1,\X_{\frac{n}{2}+1}=0,\dotsc,\X_n=0 \mid \frac1n\sum\X_i = \frac12] = \Pr[\X=x \mid \frac1n\sum\X_i = \frac12] = \frac{1}{\binom{n}{n/2}} \]
  for all $x\in\bits{n}$ such that $\frac{n}{2}$ entries are 1.

  More generally, suppose $x$ has exactly $k$ ones where $k = n\bar\theta$. Then,
  \[
    \Pr[\X=x \mid \frac1n\sum\X_i = \bar\theta] = \begin{cases}
      1/\binom{n}{n\bar\theta} & \frac1n\sum x_i = \bar\theta \\
      0                        & \text{otherwise}
    \end{cases}
  \]
  so we have that $\X \mid \frac1n\sum\X_i = \bar\theta$ is independent of $\theta$.

  We can also see this by saying that $\X \sim \Bern(\theta)^n$
  can be equivalently sampled as:
  \begin{enumerate}
    \item first sampling $\rv K = k$ with probability $\Pr[\frac1n\sum\X_i=k]$,
    \item then sampling a uniform random point that has exactly $\rv K$ ones.
  \end{enumerate}
  which clearly shows that $\rv X$ can be sampled as $f(\frac1n\sum\X_i,\rv R)$
  for some new randomness $\rv R$ (the uniform randomness) independent of $\theta$.
\end{prf}

\begin{example}[``mostly unrelated \textit{\href{https://en.wikipedia.org/wiki/Drunken_Master_III}{Drunken Master III}}'']
  A public domain generic drunkard legally distinct from Jackie Chan begins at $(0,0)$
  and takes steps in random directions $d_i$
  of length $\ell \sim \abs{\normal(0,\theta^2)}$.
\end{example}

Let $\X_n$ be the position at time $n$.
We can show that
\[ \norm{\X_n}_2 = c(1\pm o(1))\theta\sqrt{n} \]
with probability very close to 1. To be more precise,
\[ \Pr[\text{length from origin} > (1+o(1))(\text{expected length from origin})] \]
is exponentially small in $n$.
That is, after $n$ steps, the randomness cancels out,
and we have a pretty good idea of where we end up.

The whole point of this exercise is to notice that if we have a sufficient statistic,
the probability measure is extremely concentrated around some constant,
and we can almost just treat the statistic as a constant itself.

\begin{example}
  Consider \iid Gaussians $\X_1,\dotsc,\X_n \sim \normal(0,1)$.
  Then, what is the probability $\Pr[\X_1,\dotsc,\X_n > t\sqrt{n}]$
  we overshoot the estimator by $t$ times?
\end{example}
\begin{sol}
  Apply simple properties of Gaussians from STAT 230:
  \begin{align*}
    \Pr[\X_1,\dotsc,\X_n > t\sqrt{n}] = \Pr[\sqrt n\normal(0,1) > t\sqrt{n}] = \Pr[\normal(0,1) > t] = \Phi(t) \approx e^{-t^2/2}
  \end{align*}
\end{sol}

\begin{lemma}[rotation invariance of the Gaussian]
  Let $\X$ be a Gaussian and $O$ be an orthonormal matrix.
  Then, $O\X$ is distributed identically to $\X$.
\end{lemma}
\begin{prf}[super sketchy]
  Consider \iid $\X_1,\dotsc,\X_n \sim \normal(0,1)$.
  Then, since $p(x_i) = \frac1{\sqrt{2\pi}}\exp(-\frac{x_i^2}{2})$, we have
  \[ p(x_1,\dotsc,x_n) = \frac{1}{\sqrt{2\pi}^n}\exp(-\frac{\norm{x}_2^2}{2}) \]
  Notice that this only depends on the length of $x$,
  so we are uniformly distributing on the $n$-ball of length $\norm{x}_2$.
\end{prf}

Now consider what's going on with a summation.
Notice that $\sum \X_i = \ev{\X, \bb 1}$.
There exists some rotation $O$ such that $O\bb 1 =\sqrt{n}e_1$ (the first basis vector).
Inner products preserve rotations, so $\sum \X_i = \ev{O\X,O\bb 1} = \sqrt{n}\ev{O\X,e_1} = \sqrt{n}O\X_1$.
But by rotation invariance, this has the same distribution as $\sqrt{n}\X_1$,
which is just a Gaussian.

\lecture{June 17}
\section{Concentration of measure}
\begin{defn*}[shapes!]
  Let $B^n$ be the unit $n$-ball, i.e., $\{x\in\R^n : \norm{x}_2 \leq 1\}$.

  Also write $S^n$ for the unit $n$-sphere, i.e., $\partial B^n = \{x\in\R^n : \norm{x}_2 = 1\}$.
\end{defn*}

\begin{problem}[isoparametric problem]
  Of all $A \subseteq \R^n$ such that $\text{Vol}_n(A) = 1$,
  what $A$ minimizes $\text{Vol}_{n-1}(\partial A)$?
\end{problem}

It can be shown that $A = B^n$ with minimal ``surface area''.
The proof uses symmetrization:
take an axis, then align all the perpendicular fibres to that axis.

Very similarly, we can show that of all $n-1$ dimensional shapes lying
on the $n-1$-sphere, the one that minimizes the spherical volume is the hemispherical cap.
The proof uses a spherical version of symmetrization
where great circles take the place of lines.

Since we know that with stupidly high probability,
$n$ \iid Gaussians will end up on a sphere with radius $\sqrt{n}$,
we can consider the ``Gaussian isoparametric problem''.

Suppose again that $\rv G_i \sim \normal(0,1)$ and $g_i$ is a sample.
If we imagine the vector $g$ lying on an $n$-ball,
we showed that $\norm{g}_2 \approx \sqrt{n}$ with exponential decay.

\begin{defn}[Lipschitz continuity]
  A function $f : \R^n \to \R^m$ is \term*{$L$-Lipschitz continuous}
  if for all $x$ and $y$ in $\R^n$, $\abs{f(x) - f(y)} \leq L\norm{x-y}_2$.
\end{defn}

\begin{theorem}[Gaussian concentration inequality]\label{thm:gci}
  Let $f$ be any 1-Lipschitz function $f : \R^n \to \R$ and for all $x$, $y$,
  $\abs{f(x)-f(y)} \leq \norm{x-y}_2$. Then,
  \[ \Pr(\abs{f(g) - \text{median}(f(g))} > t) \]
  is exponentially small in $t$ and $n$.
\end{theorem}

We will not prove this rigorously.
Milman used this to show that for any convex body $C$ in John position
that is origin-symmetric, a random subspace $S$ of dimension $\log n$
``looks like a ball'' with probability $1-e^{-n}$.
That is, $(1-\varepsilon)B^n \subseteq S \cap C \subseteq (1+\varepsilon)B^n$
for $\varepsilon \to 0$ as $n \to \infty$.

\begin{prf}[sketchier than the Tenderloin]
  We can pretend that the Gaussian variable just lies on $\sqrt{n}S^{n-1}$.
  Define \[ A = \{x\in S^{n-1} : f(x) \geq \median_{z\sim S^{n-1}} f(z)\} \]
  such that \[ \Pr_{\X\sim S^{n-1}}[\X\in A] = \frac12 \]
  Also define a slight relaxation of $A$
  \[ A_\varepsilon = \{x\in S^{n-1} : f(x) \geq \median_{z\sim S^{n-1}} f(z) - \varepsilon\} \]
  We can show ``fairly easily'' that
  \[ \lim_{\varepsilon\to0}\frac{\Pr[\X\in A_e]-\Pr[\X\in A]}{\varepsilon} = 0 \]
  and we can relate $\Pr[\abs{f(x)-\median f(x)} \geq \varepsilon]$
  to the spherical volume of $\partial A$.

  [something something isoparametric] $A$ must be a hemisphere.

  A hemisphere can be written as $\{ x \in S^{n-1} : \ev{x,a} \geq 0 \}$.

  Then, we can rephrase the question as
  $\Pr[\sum x_i \in [-\varepsilon,\varepsilon]]$ being exponentially small
  which we already showed?
\end{prf}

\chapter{Coding theory}

\section{Source coding}

Shannon used a nice choice of $f$ with the \nameref{thm:gci}
to develop his channel coding theorem.

Let $\X$ be a random variable over a finite sample space $\XX$.
On average and in expectation, we need $H(\X)+\varepsilon$ bits/symbol
to store symbols $\X_1,\dotsc,\X_n$. Then, we can say that
\[
  \lim_{n\to\infty}\Pr[\abs{\frac1n\sum \log p(\X_i) - H(\X)} > \varepsilon] = 0 \nr{eq:logclose}
\]
for any fixed $\varepsilon$ (via the central limit theorem).

We can define a typical set $A_\varepsilon$ where \cref{eq:logclose} fails
(i.e., $\abs{\cdots} \leq \varepsilon$ and the probability does not go to zero)
and use naive 0/1 encodings of those $\abs{A_\varepsilon} = 2^{H(\X)n + \varepsilon n}$ common strings.
Encode the remainder with garbage long strings,
and we experience no penalty since we still have
an expected length $H(\X)n + \varepsilon n$.

\lecture{June 19}
\begin{theorem}[Shannon's source coding theorem]
  Let $\X \sim (p(x))$ be a random variable over a finite sample space $\XX$.
  Let $\X_1,\dotsc,\X_n$ be \iid copies of $\X$.

  For all $\varepsilon > 0$ and any sufficiently large $n$,
  there exists an encoding map $E : \XX^n \to \bits*$
  such that the performance of $E$
  \[ \text{perf}(E) = \frac1n\E\abs{E(\X_1,\dotsc,\X_n)} \leq H(\X) + \varepsilon \]
  Further, for all encoding maps $E : \XX^n \to \bits*$,
  \[ \text{perf}(E) \geq H(\X) \]
\end{theorem}

Consider an example: let $\XX = \{1,\dotsc,\Sigma\}$ and $\X = \Unif(\XX)$.
Then, $H(\X) = \log_2\Sigma$.
If this is an integer, we can encode using the base-2 encoding of $\XX$
so that
\[
  \text{perf}(E)
  = \abs{E(x_1,\dotsc,x_n)} = \abs{B_2(x_1)\cdots B_2(x_n)}
  = (\log_2\Sigma)n = nH(\X)
\]
for any sample.

Otherwise, interpret $x \in \XX^n$ as a base-$\Sigma$ number
and convert \emph{that} to base-2 so that
\[
  \text{perf}(E)
  =\abs{E(x)}
  = \abs{B_2\qty(\sum_i \Sigma^{i-1} (x_i-1))}
  \leq \abs{B_2(\Sigma^n)}
  = \ceil{n\log_2\Sigma}
  = \ceil{nH(\X)}
\]
The key observation of Shannon's theorem
is that taking copies of \emph{any} random variable will eventually
``look a lot like'' a uniform random variable over some special set.

The intuition here is that if we have a very large $n$,
the proportions are pretty much fixed and you have a uniform distribution
over the permutations.

Recall the central limit theorem. Then,

\begin{corollary}
  For all $\varepsilon > 0$ and any $n \in \N$,
  if $\X = (\X_1,\dotsc,\X_n)$ are \iid, then
  \[ \Pr[\abs{\frac1n\sum_{i=1}^n\log_2\frac{1}{p(\X_i)} - H(\X)} > \varepsilon] \leq o(1) \]
\end{corollary}
Notice that all the $p(\X_i)$'s are identical since the $\X_i$'s are \iid.
Then, $\E[\log_2\frac{1}{p(\X_i)}] = H(\X)$ so what we are really measuring
is the difference between theoretical entropy and the ``entropy'' of the sample.

\newcommand{\aeps}{A_{(\varepsilon)}^{(n)}}
\begin{defn}
  Let
  \begin{align*}
    \aeps
     & \coloneqq \qty{x\in\XX^n \Bigm| H(\X) - \varepsilon \leq \frac1n\sum\log_2\frac{1}{p(x_i)} \leq H(\X) + \varepsilon}       \\
     & = \qty{x\in\XX^n \Bigm| \frac{1}{2^{(H(\X)+\varepsilon)n}} \leq p(x_1,\dotsc,x_n) \leq \frac{1}{2^{(H(\X)-\varepsilon)n}}}
  \end{align*}
  be the set of values that work nicely with the CLT corollary,
  or, equivalently, the set of strings that have an equal-ish probability.
\end{defn}

Quick exercise: we have a random variable $\rv R$ over $[\Sigma]$,
and we know that $\frac{1}{k^{1+\varepsilon}} \leq \Pr[\rv R=i] \leq \frac{1}{k^{1-\varepsilon}}$.
Then, we can bound $\Sigma$ by $k^{1-\varepsilon} \leq \Sigma \leq k^{1+\varepsilon}$.
This is because if we had more/symbols, the sum of probabilities would be over/under 1.

If $\rv R$ instead could take other values with probability at most $\varepsilon$,
then our upper bound on $\Sigma$ remains the same and the lower bound
becomes $(1-\varepsilon)k^{1-\varepsilon} \leq \Sigma$.

\begin{claim}
  $(1-o(1))2^{(H(\X)+\varepsilon)n} \leq \abs{\aeps} \leq 2^{H(\X)n}$
  assuming $H(\X) \geq 1$.
\end{claim}
Informally: we can divide the possibilities into $\aeps$,
where the distribution looks uniform, and $\XX \setminus \aeps$,
which has vanishingly small probability.

Now, we can naively encode $\aeps$ using the binary numbers up to $\abs{\aeps}$.

Encode the rest of $\XX^n$ using larger numbers.

The performance of this encoding is
\begin{align*}
  n\operatorname{perf}(E) & = \E_{\X_1,\dotsc,\X_n} \abs{E(\X_1,\dotsc,\X_n)}                             \\
                          & = \Pr[(\X_1,\dotsc,\X_n) \in \aeps]\cdot\ceil{\log_2 A^{(n)}_{(\varepsilon)}}
  + \Pr[(\X_1,\dotsc,\X_n) \not\in \aeps]\cdot\ceil{\log_2 \abs{\XX}^n}                                   \\
                          & = (1-o(1))\ceil{(H(\X)+\varepsilon)n} + o(1)\ceil{n\log_2\abs{\XX}}
\end{align*}
In particular here it turns out that the $o(1)$ term is around about $\frac{1}{\sqrt{n}}$ and
\[ \text{perf}(E) = H(\X)n + \varepsilon n + \sqrt{n}\log\abs{\XX} \]
This does not actually help us in real life,
since sorting the symbols by their presence in $\aeps$
would take $2^n$ space.

\section{Channel coding}
\lecture{June 24}

Shannon set up a noise model where:
\begin{itemize}[nosep]
  \item Start with a source message.
  \item Encode it in some alphabet $\XX$.
  \item As it goes through the noisy channel $\XX \to \YY$,
        apply a probabilistic mapping to each character independently.
        Every $x \in \XX$ is mapped to a random $y \in\YY$ with some probability.
  \item The message is decoded from $\YY$.
  \item We must recover the original message with high probability.
\end{itemize}

\begin{defn}[binary symmetric channel]
  The \term*{binary symmetric channel} $\BSC(p)$
  is as described above where $\XX = \YY = \{0,1\}$
  such that $b \mapsto 1-b$ with probability $p$
  and $b \mapsto b$ with probability $1-p$.
\end{defn}

We can now state Shannon's channel coding theorem.

\begin{theorem}[Shannon's channel coding theorem for $\BSC(p)$]\label{thm:scc}
  For all $0 \leq p < \frac12$ and $0 < \varepsilon < \frac12-p$
  with $n$ sufficiently large:
  \begin{itemize}
    \item Possibility: For all $k \leq \floor{(1-H(p+\varepsilon))n}$,
          there exists $E : \bits{k} \to \bits{n}$ and $D:\bits{n}\to\bits{k}$
          such that
          \[ \forall m \in \bits{k} \qc \Pr_{\rv e\sim B(p)^n}[D(E(m)+\rv e) = m] \geq 1-\frac{1}{2^{\Omega(n)}} \]
    \item Impossibility: If $k \geq \ceil{(1-H(p)+\varepsilon)n}$,
          then for every pair of encoding and decoding maps
          $E : \bits{k} \to \bits{n}$ and $D : \bits{n} \to \bits{k}$,
          \[ \exists m\in\bits{k} \qc \Pr_{\rv e\sim B(p)^n}[D(E(m)+\rv e) = m] \leq \frac12 \]
  \end{itemize}
\end{theorem}

Before we prove this, let's develop a proof technique using geometric intuition.

\section{Interlude: Packing, covering, and Voronoi tiling}
Consider the space $\bits{n}$.
Then, by our discussion of concentration of measure, $\E[m]+\rv e$
lives with extremely high probability inside a ``ball''.
In particular, $\rv e$ is in the \term{Hamming ball} of radius $(p+\varepsilon)n$,
\[ \Ball_H((p+\varepsilon)n) = \{ e : \abs{e}_H \leq (p+\varepsilon)n \} \]
where $\abs{e}_H$ is the number of non-zero entries in $e$.\footnote{
  We may think of the Hamming wall as a 0-norm ball.}

Now, consider this deterministic form of \nameref{thm:scc}:

\begin{prop}
  Possibility: There exists encoding/decoding maps $E : \bits{k} \to \bits{n}$
  and $D : \bits{n} \to \bits{k}$ such that for all messages $m \in \bits{k}$,
  there exists a subset $S_m$ of the Hamming ball $\Ball_H((p+\varepsilon)n)$
  of fractional size
  \[ \frac{\abs{S_m}}{\abs{\Ball_H((p+\varepsilon)n)}} = 1-\frac{1}{2^{\Omega(n)}} \]
  such that for all errors $e \in S_m$,
  \[ D(E(m)+e) = m \]
  Impossibility: for all $E$, $D$, and $m$, it cannot be that the correctly encoded vectors
  $S_m = \{e : D(E(m)+e) = m\}$ have fractional size
  \[ \frac{\abs{S_m}}{\abs{\{\}}} \geq \frac12 \]
\end{prop}

Consider any $m$ and let $D_m$ be the set of vectors decoded to $m$.
Geometrically, in $\bits{n}$, the space must be partitioned according to the $D_m$'s.

\begin{remark}
  If we have a shape $S \subseteq \sv S$ we would like to tile the space $\sv S$ with,
  the \term{volume ratio} $\frac{\abs{S}}{\abs{\sv S}}$ is an upper bound
  on the number of $S$'s you can fit inside $\sv S$.
\end{remark}

In high dimensions, for almost all shapes, this also becomes a good estimate.
In particular, it works for the Hamming ball.

\begin{remark}
  For the Hamming ball, there is a Euclidean ball of appropriate radius
  such that in almost all directions, the two balls look the same.
\end{remark}

\begin{prf}[of the impossibility direction]
  $D_m$ has large intersection with a large set $E(m) + \Ball_H((p+\varepsilon)n)$

  So $D_m$ must have high volume.

  But the $D_m$ shape tiles the space $\bits{n}$ with finite volume.

  Therefore, $k$ cannot be too large otherwise there are too many $D_m$'s.
\end{prf}

\lecture{June 26}

TODO

\lecture{July 3}

\begin{defn}
  Fix some small fixed constant $\varepsilon > 0$.
  Let $p \in (0,\frac12)$, $k = (1-H(p)-\varepsilon)n$, and $n$ grows to $\infty$.

  The encoding map $E : \F_2^k \to \F_2^n$ is defined randomly and independent message-wise.

  Let $D : \F_2^n \to \F_2^k$ be the nearest-codeword map associated to $E$.
\end{defn}

We will delete a fraction $2^{-\varepsilon'n}$ of codewords that are bad.

\begin{defn}
  An element $E(m)$ is \term*{bad} if
  $\frac{(E(m) + \Ball_H(pn)) \cap D_m}{\Ball_H(pn)} \leq 1-2^{-\varepsilon''n}$
  for $\varepsilon'' = ?$ (fill in later).

  We call another element $y$ \term*{corrupt} if it causes $E(m)$ to be bad.
\end{defn}

\begin{defn}
  The \term{Voronoi cell} $D_m = \{y \in \F_2^n : d_H(y,m) = \min_{m'\in \F_2^k} d(y,m')\}$.
\end{defn}

\begin{claim}
  Fix an $E(m)$ and a $y$ in its Hamming ball $E(m) + \Ball_H(pn)$. Then,
  \[
    \Pr_{E(m')}[d(y,E(m')) < d(y,E(m))] \leq \frac{1}{2^k}\cdot\frac{1}{2^{\varepsilon n}}
  \]
\end{claim}
\begin{prf}
  \begin{align*}
    \Pr_{E(m')}[d(y,E(m')) < d(y,E(m))]
     & \leq \Pr_{E(m')}[E(m') \in y+\Ball_H(pn-1)]     \\
     & = \frac{\Vol(\Ball_H(pn-1))}{\abs{F_2^n}}       \\
     & \leq \frac{2^{H(p)n}}{2^n}                      \\
     & = \frac{1}{2^k}\cdot\frac{1}{2^{\varepsilon n}}
  \end{align*}
\end{prf}

Now, what is the probability a codeword is bad?

\begin{claim}
  Fix an $E(m)$ and a $y$ in its Hamming ball $E(m) + \Ball_H(pn)$.
  Of the remaining encoding maps,
  \[
    \Pr_{E : \F_2^k \setminus \{m\} \to \F_2^n} [\text{$y$ is corrupt}]
    \leq \frac{1}{2^{\varepsilon n}}
  \]
\end{claim}
\begin{prf}
  \begin{align*}
    \Pr_{E : \F_2^k \setminus \{m\} \to \F_2^n} [\text{$E(m)$ is bad}]
     & \leq \sum_{m' \neq m} \Pr_{E : \F_2^k \setminus \{m\} \to \F_2^n}[\text{$E(m')$ is closer to $y$ than $E(m)$}] \\
     & = \sum_{m' \neq m} \Pr_{E(m')}[\text{$E(m')$ is closer to $y$ than $E(m)$}] \tag{by independence}              \\
     & \leq 2^k \cdot \frac{1}{2^k} \cdot \frac{1}{2^{\varepsilon n}}                                                 \\
     & = \frac{1}{2^{\varepsilon n}}
  \end{align*}
\end{prf}

\paragraph{Aside: Markov's inequality} Let $a_1,\dotsc,a_n \in \R_{\geq 0}$.
Suppose $\frac1n\sum a_i \leq \lambda$.
Can $\varepsilon n$ indices $a_i$ satisfy $a_i > \frac\lambda\varepsilon$?
No! Otherwise, $\sum a_i \geq \sum_{\text{big}} a_i > \varepsilon n \cdot \frac{\lambda}{\varepsilon} > n \lambda$.
Take this principle and generalize.

\begin{theorem}[Markov's inequality]
  Let $\X$ be a random variable. Fix $\varepsilon > 0$. Then,
  \[ \Pr_{\X}\qty[\X > \E[\X] \cdot \varepsilon] \leq \varepsilon \]
\end{theorem}

\begin{claim}
  Fix any $m$. Then,
  \[
    \Pr_{E : \F_2^k \to \F_2^n} [\text{$E(m)$ is bad}] \leq \frac{1}{2^{\varepsilon n / 2}}
  \]
\end{claim}
\begin{prf}
  \begin{align*}
    \Pr_{E : \F_2^k  \setminus \{m\} \to \F_2^n} [\text{$E(m)$ is bad}]
     & = \Pr_{E : \F_2^k \to \F_2^n} [\text{$E(m)$ is bad}] \tag{by independence}                                                        \\
     & = \Pr_{E}\qty[\abs{\{y \in \Ball_H(pn) + E(m) : \text{$y$ is corrupt}\}} > \frac{1}{2^{\varepsilon n/2} \cdot \abs{\Ball_H(pn)}}] \\
     & \leq \frac{1}{2^{\varepsilon n/2}} \tag{Markov's inequality}
  \end{align*}
  because
  \begin{align*}
    \E[\abs{\{y \in \Ball_H(pn) + E(m) : \text{$y$ is corrupt}\}}]
    \leq \frac{1}{2^{\varepsilon n}}\abs{\Ball_H(pn)}
  \end{align*}
\end{prf}

\begin{prop}
  With probability $1-o(1)$, there are at most $\abs{\F_2^k}2^{-\varepsilon n/4}$ bad codewords.
\end{prop}
\begin{prf}
  \begin{align*}
    \Pr_E\qty[\abs{\{m : \text{$E(m)$ is bad}\}} > \frac{1}{2^{\varepsilon n/4}}\cdot\abs{\F_2^k}]
     & \leq \frac{\E[\abs{\{m : \text{$E(m)$ is bad}\}}]}{\abs{\F_2^k}/2^{\varepsilon n/4}} \\
     & \leq \frac{2^{\varepsilon n/4}}{2^k} \cdot \frac{2^k}{2^{\varepsilon n/2}}           \\
     & = \frac{1}{2^{\varepsilon n/4}}
  \end{align*}
  Therefore, with probability $1-2^{-\varepsilon n/4} = 1-o(1)$,
  there are at most $\abs{\F_2^k}2^{-\varepsilon n/4}$ bad codewords.
\end{prf}

\begin{problem}
  We have shown an almost-packing of Hamming balls in $\F_2^n$.

  What about an exact packing of Hamming balls?
  This is an open question.

  What about in a continuous field?
\end{problem}

\section{Shannon's channel coding for discrete memoryless channels}
\lecture{July 8}

Consider a message $m \in \bits k$ deterministically encoded into a sentence
$E(m) = X = (x_1,\dotsc,x_n) \in \XX^n$.
Then, we send the words through a noisy channel $\XX \to \YY$
where each symbol $x_i \mapsto y_j \in \YY$ with some probability $p(y_j \mid x_i)$.
This results in a random sentence $\rv Y$.
We try to decode $D(Y) = \hat m$.

\begin{defn}
  The \term*{performance} of an encoding/decoding pair $(E,D)$ is
  \[ \perf(E,D) = \min_{m\in\F_2^k} \Pr_{p}[D(\rv Y \mid m) = m] \]
\end{defn}

We will show a scheme that gives performance $\geq 1-\varepsilon$.

Fix any marginal distribution $(p(x))_{x\in\X}$.
Then, let $\X$ be the random variable taking $x$ with probability $p(x)$.
Let $\Y$ be the channel noising of $\X$.
We will consider the joint distribution $(\X,\Y)$.

\begin{defn}
  Given a channel $(p(y \mid x))_{x\in\XX,y\in\YY}$, the \term{channel capacity}
  \[ C := \sup_{\mathclap{(p(x))_{x\in\XX}}} I(\X : \Y) \]
\end{defn}

For $\BSC(p)$, $C = 1-H(p)$ as expected.

Channel capacity characterizes the length of messages that can be sent.

\begin{theorem}[Shannon's channel coding for discrete memoryless channels]
  Fix $\varepsilon > 0$. For $n$ sufficiently large, if $\frac k n < C$,
  there exists $(E, D)$ such that $\perf(E,D) \geq 1-\varepsilon$.

  If $\frac k n \geq C$, then no such $(E, D)$ exists.
\end{theorem}
\begin{prf}
  Consider the following encoding scheme:
  \begin{quote}
    Pick marginal $(p(x))_{x\in\XX}$ that attains $C$ and fix it.
    We will pick $2^k$ \iid random codewords $\X_i$ in $\XX^n$
    according to the distribution $\X_i \sim p^n$,
    then assign them to the messages $m \in \bits{k}$.
  \end{quote}
  Let $X,X' \in \XX^n$ be random codewords.
  Let $Y \in \YY^n$ be the random sentence received if $X$ is sent over the channel.
  How likely does $Y$ get decoded to $X'$?

  First, what does $(X,Y) \in \XX^n \times \YY^n$ look like?
  Start by considering $X \in \XX^n$.
  It is almost certainly uniformly distributed over the typical set $\XX_\varepsilon^{(n)}$.
  Likewise, $Y$ is almost certainly uniformly distributed over the typical set $\YY_\varepsilon^{(n)}$.

  Using the fact that CLT needs only independence and \emph{not}
  identical distributions, we similarly get that $(X,Y)$
  is roughly uniformly distributed over a ``typical set'' of some sort.

  Formally: suppose that
  \begin{gather*}
    \XX_\varepsilon^{(n)} = \{ X \in \XX^n : 2^{-H(x)n(1+\varepsilon)} \leq \Pr[\X=X] \leq 2^{-H(x)n(1-\varepsilon)}\} \\
    \YY_\varepsilon^{(n)} = \{ Y \in \YY^n : 2^{-H(y)n(1+\varepsilon)} \leq \Pr[\Y=Y] \leq 2^{-H(y)n(1-\varepsilon)}\}
  \end{gather*}
  then we can write
  \begin{align*}
    \XX\YY_{\varepsilon}^n = \{
     & (X,Y) \in \XX^n\times\YY^n : X \in \XX_\varepsilon^{(n)} \land Y \in \YY_\varepsilon^{(n)} \land \\
     & \qquad 2^{-H(x,y)n(1+\varepsilon)} \leq \Pr[(\X,\Y) = (X,Y)] \leq 2^{-H(x,y)n(1-\varepsilon)}\}
  \end{align*}
  \lecture{July 10}
  To be precise, observe that
  \begin{align*}
    \Pr[(\X,\Y) = (X,Y)]      & = \prod_{i=1}^n p(X_i, Y_i)     \\
    \log \Pr[(\X,\Y) = (X,Y)] & = \sum_{i=1}^n \log p(X_i, Y_i)
  \end{align*}
  because the pairwise $\X_i$ and $\Y_i$ are independent (but not the whole $\X$ and $\Y$).

  Then, let $\rv R$ be the random variable $\log p(x,y)$ with probability $p(x,y)$
  and let $\rv R_1,\dotsc,\rv R_n \sim \rv R$.
  Observe that $\sum \rv R_i$ has the same distribution as $\log \Pr[(\X,\Y)=(\tilde X,\tilde Y)]$.
  We can then apply the central limit theorem to say
  \[ \Pr[\abs{\frac1n\sum_{i=1}^n \rv R_i - \E[\rv R]} > \varepsilon] \leq \varepsilon' \]
  % By definition of joint entropy, $\E[\rv R] = -H(x,y)$, so we have
  % \[ \Pr[\sum_{i=1}^n \rv R_i - (-H(x,y))n]\]
  % which is equivalent to saying
  % \[ \Pr_{X,Y}\qty[\frac{\Pr_{\X,\Y}[(\X,\Y)=(X,Y)]}{2^{H(x,y)n}} \not\in [2^{-\varepsilon H(x,y)n},2^{\varepsilon H(x,y) n}]] \leq \varepsilon' \]
  % The consequences of this CLT application are:
  % \begin{enumerate}[(i)]
  %   \item $\Pr[(\X,\Y) \in \XX\YY_\varepsilon^{(n)}] = 1-o(1)$
  %   \item $\abs{\XX\YY_\varepsilon^{(n)}} \approx 2^{H(x,y)n}$
  %   \item $\Pr_{(x,y),x'}[(\X',\Y) \in \XX\YY_\varepsilon^{(n)}] \approx \frac{2^{H(x,y)n}}{2^{(H(x)+H(y))n}} = 2^{-I(x:y)n}$
  % \end{enumerate}

  For a codeword $X \in \XX^n$, define its \term*{neighbourhood} by
  \[ \neigh(X) = \{Y \in \YY^n : (X,Y) \in \XX\YY_{\varepsilon}^{(n)}\} \]
  and now by (some combination of corollaries from the CLT discussion),
  \[ \Pr_{(\X,\Y)}[\Y \not\in \neigh(\X)] \leq \varepsilon' \]
  and $(\X,\Y)$ is nearly uniform within the neighbourhood.

  Now, specify the decoding scheme.
  \begin{quote}
    Find any $\X_i$ in our list of codewords
    such that the received $Y \in \neigh(\X_i)$
    and return the message underlying $i$.
  \end{quote}
  Fix a codeword $\X_i$.
  Notice that $\abs{\neigh(\X_i)} \leq \abs{\XX\YY_\varepsilon^{(n)}} = 2^{H(x,y)n}$.
  Also, we can bound
  \begin{align*}
    \E_{\X_1,\dotsc,\X_n}\Pr_{(\X_i,\Y)}[\Y \in \neigh(\X_i)] \leq \frac{2^{(H(x,y)-\varepsilon) n}}{2^{(H(x)+H(y))n}} \leq 2^{-(I(x:y)+\varepsilon)n}
  \end{align*}
  and 
  \begin{align*}
    \E_{\X_1,\dotsc,\X_n}\Pr_{(\X_i,\Y)}[\Y \in \neigh(\X_j)] \leq \frac{2^k}{2^{(I(x:y) + \varepsilon)n}} \leq 2^{-\varepsilon n}
  \end{align*}
  because $k < I(x:y) n$. 
  Then, we can say that at most $2^{-\varepsilon n/2}$ of the $\X_i$'s have
  \[
    \E_{\X_1,\dotsc,\X_k} \Pr_{(\X_1,\Y)}[\Y \in \neigh(\E_j)] > \frac{1}{2^{\varepsilon n/2}}
  \]
\end{prf}

\chapter{Parallel repetition}

\pagebreak
\phantomsection\addcontentsline{toc}{chapter}{Back Matter}
\renewcommand{\listtheoremname}{List of Named Results}
\phantomsection\addcontentsline{toc}{section}{\listtheoremname}
\listoftheorems[ignoreall,numwidth=4em,onlynamed={theorem,lemma,corollary,prop}]
\printindex

\end{document}
