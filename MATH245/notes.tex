\documentclass[notes,tikz]{agony}

\title{MATH 245 Spring 2025: Lecture Notes}

\begin{document}
\renewcommand{\contentsname}{MATH 245 Spring 2025:\\{\huge Lecture Notes}}
\thispagestyle{firstpage}
\tableofcontents

Lecture notes taken, unless otherwise specified,
by myself during the Spring 2025 offering of MATH 245,
taught by Blake Madill.

\begin{multicols}{2}
  \listoflecture
\end{multicols}

\chapter{Introduction}
\lecture{May 5}
\section{?}
\section{?}

\lecture{May 7}
Recall: for $T : V \to V$
and an ideal $I = \{f(x) \in F[x] : f(T) = 0 \}$.
\begin{fact}\label{fact:f1}
  If $\dim V < \infty$, then $I \neq \{0\}$.

  Moreover, $I = \ev{m(x)}$
  for a unique, monic, minimal polynomial $m(x)$.
\end{fact}
\begin{remark}
  $m(x)$ has minimal degree in $I$.
  If $f(x) \in I$, then $m(x) \mid f(x)$.
\end{remark}

\section{$T$-invariance}

If $T : V \to V$ is linear and $W \leq V$ we want to consider
$T : W \to W$ for inductive purposes.
\begin{defn}
  Let $T : V \to V$, $W \leq V$.
  We say $W$ is \term{$T$-invariant} if $T(W) \subseteq W$
  (i.e., $W$ is closed under $T$).

  In this case, $T_W = T|_W : W \to W$ is well-defined.
\end{defn}
\begin{example}
  Let $T : \R^2 \to \R^2$ where $T(x,y) = (2x+3y, 4x+2y)$.

  Is $W = \Span\{(1,1)\}$ $T$-invariant?
\end{example}
\begin{sol}
  No. Notice that $T(1,1) = (5,6) \not\in W$.
\end{sol}
\begin{example}
  Let $T : V \to V$ and $\lambda \in F$.
  The eigenspace $E_\lambda = \{v \in V : T(v) = \lambda v\}$
  \emph{is} $T$-invariant.
\end{example}
\begin{prf}
  Let $v \in E_\lambda$ so that $T(v) = \lambda v$.
  We want to show $T(v)$ is also an eigenvector.

  Then, $T(T(v)) = T(\lambda v) = \lambda T(v)$,
  which means $T(v) \in E_\lambda$.
\end{prf}
\begin{prop}[characteristic of the restriction divides characteristic of the operator]
  Suppose $\dim V < \infty$ and $W \leq V$ is $T$-invariant
  where $T : V \to V$.

  Let $f(x)$ and $g(x)$ be the characteristic polynomials for $T$ and $T_W$.

  Then, $g(x) \mid f(x)$.
\end{prop}
\begin{prf}
  Let $\beta = \{v_1,\dotsc,v_k\}$ be a basis for $W$.
  Extend it to a basis $\gamma = \{v_1,\dotsc,v_k,\dotsc,v_n\}$ for $V$.

  Let $A = [T_W]_\beta$.\footnote{The matrix of $T_W$ with respect to the basis $\beta$.}
  Then, $[T]_\gamma = \mqty[{[T(v_1)]_\gamma} & \dots & {[T(v_n)]_\gamma}]$.

  But since $T(v_i) \in W$, $[T(v_i)]_\gamma = [T(v_i)]_\beta$.

  Thus, $[T]_\gamma = \mqty[A & \star \\ 0 & B]$ for some submatrices $\star$ and $B$.

  Therefore, $f(x) = \det(xI - [T]_\gamma) = \det(xI-A)\det(xI-B)$.
  But $\det(xI-A)$ is just $g(x)$.
\end{prf}

\begin{defn}
  Let $T : V \to V$ be a linear operator and $v \in V$.
  We call $W_{T,v} = \Span\{v,T(v),T^2(v),\dotsc\}$
  the \term{$T$-cyclic subspace of $V$ generated by $v$}.
\end{defn}

\begin{remark}
  $W_{T,v}$ is $T$-invariant, since $T(T^i(v)) = T^{i+1}(v) \in W_{T,v}$.

  $W_{T,v}$ is the smallest $T$-invariant subspace of $V$ which contains $v$.
\end{remark}

\begin{prop}\label{prop:t-cyclic-basis}
  Let $T : V \to V$ and $v \neq 0$.
  Say $\dim W_{T,v} = k < \infty$.

  Then, $\beta = \{v,T(v),\dotsc,T^{k-1}(v)\}$ is a basis.
\end{prop}
\begin{prf}
  Let $j \in \N$ be the maximal index such that $\{v,T(v),\dotsc,T^{j-1}(v)\}$
  is linearly independent.
  This must exist since $v \neq 0$, so $\{v,T(v)\}$ is linearly independent.

  Let $U = \Span\{v,T(v),\dotsc,T^{j-1}(v)\}$.
  We must show $U = W := W_{T,v}$.
  Clearly, $U \subseteq W$.

  Consider $T^j(v)$.
  Since $j$ is maximal, $T^j(v) = a_0v + a_1T(v) + \dotsb + a_{j-1}T^{j-1}(v)$
  where at least one $a_i \neq 0$
  Hence, $T^j(v) \in U$.
  In fact, $T^i(v)$ for all $i \geq j$ is in $U$,
  so $W \subseteq U$.
\end{prf}

\begin{prop}\label{prop:cpoly}
  Let $T : V \to V$, $W = W_{T,v}$ for $v \neq 0$.

  If $\dim W = k \leq \infty$ and $f(x) = x^k + a_{k-1}x^{k-1} + \dotsb a_1x + a_0 \in F[x]$
  is such that $f(T)(v) = 0$,
  then $f(x)$ is the characteristic polynomial of $T_W$.

  That is, if we find a $k$-degree polynomial that kills the generator,
  it must be the characteristic polynomial.
\end{prop}
\begin{prf}
  Let $\beta = \{T^i(v)\}$ be the basis from \cref{prop:t-cyclic-basis}.
  Then,
  \begin{align*}
    0      & = f(T)(v) = T^k(v) + a_{k-1}T^{k-1}(v) + \dotsb + a_1 T(v) + a_0 v \\
    T^k(v) & = -a_0 - a_1T(v) - \dotsb - a_{k-1}T^{k-1}(v)
  \end{align*}
  so $[T_W]_\beta = \mqty[0&0&\cdots&0&-a_0\\1&0&\cdots&0&-a_1\\0&1&\cdots&0&-a_2\\\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&\cdots&1&-a_{k-1}]$.

  By assignment 1, $f(x)$ is the characteristic polynomial of $[T_W]_\beta$.
\end{prf}

\section{The Cayley--Hamilton theorem}
\begin{theorem}[Cayley--Hamilton]
  Let $\dim V < \infty$ and let $T : V \to V$ be linear.

  If $f(x)$ is the characteristic polynomial for $T$, then $f(T) = 0$.

  That is, a linear operator satisfies its own characteristic polynomial.
\end{theorem}

Why do we care?

\begin{remark}
  In this case, if $f(x) \in I = \{g(x) : g(T) = 0\}$,
  then $I \neq \{0\}$.
  That means the minimal polynomial of $T$ from \cref{fact:f1} exists
  and $m(x) \mid f(x)$.
\end{remark}

\lecture{May 9}
We can now prove the Cayley--Hamilton theorem:

\begin{prf}
  If $v = 0$, then $f(T)(v) = 0$,
  so assume $v \neq 0$.

  Let $W = W_{T,v}$ and consider $V_W : W \to W$.

  Suppose the characteristic polynomial of $T_W$
  is $g(x)$ and let $\beta = \{v,T(v),\dotsc,T^{k-1}(v)\}$ be a basis for $W$.

  But since $T^k(v)$ is not in this basis,
  there exist $a_i \in F$, not all zero, such that
  \[ T^k(v) + a_{k-1}T^{k-1}(v) + \dotsb + a_1T(v) + a_0v = 0 \]
  but by \cref{prop:cpoly}, this is the characteristic polynomial $g(x)$.
  Therefore, $g(T)(v) = 0$.

  Finally, since $g(x) \mid f(x)$, we also have $f(T)(v) = 0$.
\end{prf}

\section{Properties of the minimal polynomial}

\begin{prop}
  Let $T : V \to V$ and $\dim V < \infty$.
  Let $m(x)$ and $f(x)$ be the minimal and characteristic polynomials.
  Then, $m(x)$ and $f(x)$ have the same roots in $F$.
\end{prop}
\begin{example}
  Suppose $T : V \to V$ with $f(x) = (x-2)^2(x-3)$.
  Then, $m(x)$ must be $(x-2)(x-3)$ since it has the same roots and has minimal degree.
\end{example}
\begin{prf}
  Since $m(x) \mid f(x)$, every root of $m(x)$ is a root of $f(x)$.

  Let $\lambda \in F$ be a root of $f(x)$.
  Say $T(v) = \lambda v$ for $v \neq 0$.
  Then, consider $m(T)(v)$:
  \begin{align*}
    m(T)(v) = \sum a_i T^i(v) = \sum a_i (\lambda^i v) = m(\lambda)v
  \end{align*}
  But we know $m(T) = 0$, so $m(\lambda)v = 0$.
  Finally, since $v \neq 0$, we must have $m(\lambda) = 0$.
\end{prf}

\begin{defn}
  Let $A \in M_n(F)$. The \term{minimal polynomial of $A$}
  is the unique, monic $m(x) \in F[x]$ such that $\ev{m(x)} = \{f(x) : f(A) = 0\}$.
\end{defn}
\begin{xca}
  Let $A \in M_n(F)$, $T_A : F^n \to F^n$, $T_A(x) = Ax$.
  Show that the minimal polynomial of $A$ is the minimal polynomial of $T_A$.
\end{xca}

\begin{example}
  Find the minimal polynomial of $A = \mqty[3&-1&0\\0&2&0\\1&-1&2]$.
\end{example}
\begin{sol}
  Expand and find $f(x) = (x-2)^2(x-3)$.

  Either $m(x) = f(x)$ or $m(x) = (x-2)(x-3)$.
  Since $(A-2I)(A-3I)=0$, we have $m(x) = (x-2)(x-3)$.
\end{sol}

\begin{remark}
  Jordan canonical form will provide a much better algorithm for computing
  these minimal polynomials.
\end{remark}

The minimal polynomial is already helpful: it gives us eigenvalues.
But we can do more.

\begin{prop}
  Let $T : V \to V$, $\dim V < \infty$.
  Let $\lambda_i$ be the distinct eigenvalues of $T$ in $F$.
  Then, $T$ is diagonalizable if and only if the minimal polynomial of $T$
  is $m(x) = \prod (x-\lambda_i)$.
\end{prop}

This is much faster than checking multiplicities of the characteristic polynomial
against the geometric multiplicities from the operator's eigenspaces.

\begin{prf}
  ($\Rarr$) Suppose $T$ is diagonalizable and let $\beta = \{v_i\}$
  be a basis for $V$ consisting of eigenvectors for $T$.
  Let $p(x) = \prod (x-\lambda_i)$.

  First, $p(x) \mid m(x)$ since each factor is certainly in $m(x)$.

  Now, consider a $v_i \in \beta$ with $T(v_i) = \lambda_i v_i$.
  But we know that $p(x) = q_i(x)(x-\lambda_i)$ for some $q_i(x)$.

  Then, $p(T)(v_i) = q_i(T)(v_i)\cdot (T-\lambda_i I)(v_i) = q_i(T)(v_i)\cdot(Tv_i - \lambda_i v_i) = 0$.
  Since $m(x)$ divides everything that makes $T$ vanish,
  $m(x) \mid p(x)$.

  Because $m(x)$ and $p(x)$ are monic, they must now be equal.

  ($\Larr$) Assume the minimal polynomial of $T$ is $m(x) = \prod (x-\lambda_i)$.
  Induct on the dimension.

  If $\dim V = 1$, then we are done ($1 \times 1$ matrices are diagonal).

  Assume that if $\dim V < n$, $T$ is diagonalizable.
  Take $\dim V = n$.
  Consider the subspace $W = \im_{T-\lambda_k I}(V)$.
  Since $T \circ (T-\lambda_k I) = (T-\lambda_k I)\circ T$,
  i.e., $T(\text{something in the range})$ is in the range,
  $W$ is $T$-invariant.
  Therefore, $T_W : W \to W$ is linear.

  Let $m_W(x)$ be the minimal polynomial of $T_W$.
  Then, $m(T_W) = 0$ and therefore $m_W(x) \mid m(x)$.

  Since the kernel of $T-\lambda_k I$ is non-trivial (it contains the eigenvector),
  the nullity of $T-\lambda_k I$ is non-zero and the rank is less than $n$.
  That is, $\dim W < n$ and $T_W$ is diagonalizable by the inductive hypothesis.
\end{prf}

\end{document}
